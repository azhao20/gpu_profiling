{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
    "from utils.prediction_utils import *\n",
    "\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [00:18,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< 0 found in file time.32.1024.0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:26,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< 0 found in file time.128.512.0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [00:28,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< 0 found in file time.128.1024.0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [00:34,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< 0 found in file time.512.128.0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56it [00:37,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< 0 found in file time.512.512.0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58it [00:39,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< 0 found in file time.512.1024.0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [00:40,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< 0 found in file time.512.1024.1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:43,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< 0 found in file time.1024.32.0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [00:45,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< 0 found in file time.1024.128.0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68it [00:47,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< 0 found in file time.1024.512.0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [00:49,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< 0 found in file time.1024.512.1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70it [00:49,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< 0 found in file time.1024.1024.0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [00:51,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< 0 found in file time.1024.1024.1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [00:51,  1.39it/s]\n",
      "/Users/andrew/Desktop/Harvard/idreos-research/gpu_profiling/utils/time_utils.py:402: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dfs = pd.concat(dfs, axis=0, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Load your data using the get_data function\n",
    "base_dir = \"/Users/andrew/Desktop/Harvard/idreos-research/gpu_profiling\"\n",
    "X, y = get_data(\"conv2d\", base_dir, sample_rate=1.0)\n",
    "\n",
    "# Combine X and y into a single DataFrame for convenience\n",
    "df = pd.concat([X, y], axis=1)\n",
    "df = df.query(\"time > 0\").dropna()\n",
    "X, y = df.drop(columns=[\"time\"]), df[\"time\"]\n",
    "\n",
    "# Ensure dtype columns are of integer type\n",
    "X = X.astype({\"dtype_16\": int, \"dtype_32\": int, \"dtype_b16\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_param = 0.93\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_array = X_scaled  # Use scaled features\n",
    "y_array = y.values  # Target variable as numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use roofline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Loss: 244362.9625, Validation Loss: 249649.3416\n",
      "Min predicted_alpha: 0.009999999776482582, Max predicted_alpha: 0.5288382768630981\n",
      "Epoch [2/20], Training Loss: 264880.7922, Validation Loss: 280792.1449\n",
      "Min predicted_alpha: 0.009999999776482582, Max predicted_alpha: 0.5288382768630981\n",
      "Epoch [3/20], Training Loss: 275795.9453, Validation Loss: 280799.8370\n",
      "Min predicted_alpha: 0.009999999776482582, Max predicted_alpha: 0.5288382768630981\n",
      "Epoch [4/20], Training Loss: 275701.4295, Validation Loss: 280918.3375\n",
      "Min predicted_alpha: 0.009999999776482582, Max predicted_alpha: 0.5288382768630981\n",
      "Epoch [5/20], Training Loss: 275636.6505, Validation Loss: 281054.6212\n",
      "Min predicted_alpha: 0.009999999776482582, Max predicted_alpha: 0.5288382768630981\n",
      "Epoch [6/20], Training Loss: 275596.1518, Validation Loss: 280761.9648\n",
      "Min predicted_alpha: 0.009999999776482582, Max predicted_alpha: 0.5288382768630981\n",
      "Epoch [7/20], Training Loss: 275620.0580, Validation Loss: 280900.8477\n",
      "Min predicted_alpha: 0.009999999776482582, Max predicted_alpha: 0.5288382768630981\n",
      "Epoch [8/20], Training Loss: 275578.0873, Validation Loss: 280834.3368\n",
      "Min predicted_alpha: 0.009999999776482582, Max predicted_alpha: 0.5288382768630981\n",
      "Epoch [9/20], Training Loss: 275579.5809, Validation Loss: 280777.4410\n",
      "Min predicted_alpha: 0.009999999776482582, Max predicted_alpha: 0.5288382768630981\n",
      "Epoch [10/20], Training Loss: 275580.7198, Validation Loss: 280797.6800\n",
      "Min predicted_alpha: 0.009999999776482582, Max predicted_alpha: 0.5288382768630981\n",
      "Epoch [11/20], Training Loss: 275808.3371, Validation Loss: 280770.6461\n",
      "Min predicted_alpha: 0.009999999776482582, Max predicted_alpha: 0.5288382768630981\n",
      "Early stopping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c8/q9xddvxj5pv262qffnm05sk00000gn/T/ipykernel_66573/3419124375.py:466: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Loss: 249649.3416\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_dtype_bytes(row):\n",
    "    if row[\"dtype_32\"]:\n",
    "        return 4\n",
    "    elif row[\"dtype_16\"]:\n",
    "        return 2\n",
    "    elif row[\"dtype_b16\"]:\n",
    "        return 2\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dtype in row.\")\n",
    "\n",
    "padding = 0\n",
    "output_padding = 0\n",
    "\n",
    "# Note: this is a heuristic.\n",
    "input_memory_accesses = df[\"b\"] * df[\"in_channels\"] * df[\"iH\"] * df[\"iW\"]\n",
    "kernel_memory_accesses = (\n",
    "    df[\"out_channels\"] * (df[\"in_channels\"] // df[\"groups\"]) * df[\"kH\"] * df[\"kW\"]\n",
    ")\n",
    "output_memory_accesses = df[\"b\"] * df[\"out_channels\"] * df[\"oH\"] * df[\"oW\"]\n",
    "memory_accesses = (\n",
    "    (input_memory_accesses + kernel_memory_accesses + output_memory_accesses)\n",
    "    * df.apply(get_dtype_bytes, axis=1)\n",
    "    / 1e9\n",
    ")  # in GB\n",
    "\n",
    "intensity = df[\"gflops\"] / memory_accesses\n",
    "\n",
    "# Convert to numpy arrays\n",
    "memory_accesses_array = memory_accesses.values  # NumPy array\n",
    "intensity_array = intensity.values  # NumPy array\n",
    "\n",
    "\n",
    "def get_dtype_peak_fp(row):\n",
    "    if row[\"dtype_32\"]:\n",
    "        return 156\n",
    "    elif row[\"dtype_16\"]:\n",
    "        return 312\n",
    "    elif row[\"dtype_b16\"]:\n",
    "        return 312\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dtype in row.\")\n",
    "\n",
    "\n",
    "# Compute pi for the entire dataset\n",
    "pi_array = df.apply(get_dtype_peak_fp, axis=1).values  # in GFLOPs\n",
    "\n",
    "# Define beta (DRAM bandwidth in GB/ms)\n",
    "beta = 2.03904\n",
    "\n",
    "# Split data into training and validation sets, including additional features\n",
    "(\n",
    "    X_train,\n",
    "    X_val,\n",
    "    y_train,\n",
    "    y_val,\n",
    "    memory_accesses_train,\n",
    "    memory_accesses_val,\n",
    "    intensity_train,\n",
    "    intensity_val,\n",
    "    pi_train,\n",
    "    pi_val,\n",
    ") = train_test_split(\n",
    "    X_array,\n",
    "    y_array,\n",
    "    memory_accesses_array,\n",
    "    intensity_array,\n",
    "    pi_array,\n",
    "    test_size=0.2,\n",
    "    random_state=random_seed,\n",
    ")\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "memory_accesses_train_tensor = torch.tensor(memory_accesses_train, dtype=torch.float32)\n",
    "memory_accesses_val_tensor = torch.tensor(memory_accesses_val, dtype=torch.float32)\n",
    "\n",
    "intensity_train_tensor = torch.tensor(intensity_train, dtype=torch.float32)\n",
    "intensity_val_tensor = torch.tensor(intensity_val, dtype=torch.float32)\n",
    "\n",
    "pi_train_tensor = torch.tensor(pi_train, dtype=torch.float32)\n",
    "pi_val_tensor = torch.tensor(pi_val, dtype=torch.float32)\n",
    "\n",
    "# Create TensorDatasets for training and validation sets\n",
    "train_dataset = TensorDataset(\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    memory_accesses_train_tensor,\n",
    "    intensity_train_tensor,\n",
    "    pi_train_tensor,\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    X_val_tensor,\n",
    "    y_val_tensor,\n",
    "    memory_accesses_val_tensor,\n",
    "    intensity_val_tensor,\n",
    "    pi_val_tensor,\n",
    ")\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 1024\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "def weighted_mse_loss(y_true, y_pred, weight):\n",
    "    loss = weight * (y_true - y_pred) ** 2\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def mape_loss(y_true, y_pred):\n",
    "    epsilon = 1e-8  # to prevent division by zero\n",
    "    loss = torch.abs(\n",
    "        (y_true - y_pred) / (y_true + epsilon)\n",
    "    )  # Add epsilon for stability\n",
    "    return torch.mean(loss)\n",
    "\n",
    "\n",
    "# Define the custom loss function\n",
    "def custom_loss_function(\n",
    "    predicted_alpha,\n",
    "    target_operator_time,\n",
    "    batch_memory_accesses,\n",
    "    batch_intensity,\n",
    "    batch_pi,\n",
    "    beta,\n",
    "):\n",
    "    \"\"\"\n",
    "    predicted_alpha: Output from the neural network (predicted alpha)\n",
    "    target_operator_time: Actual operator times (ground truth)\n",
    "    batch_memory_accesses: Memory accesses for the batch\n",
    "    batch_intensity: Arithmetic intensity for the batch\n",
    "    batch_pi: Peak FLOPs/sec for the batch\n",
    "    beta: Predefined DRAM bandwidth\n",
    "    \"\"\"\n",
    "\n",
    "    batch_total_flops = batch_intensity * batch_memory_accesses  # GFLOPs\n",
    "    predicted_alpha = predicted_alpha.clamp(min=1e-6)\n",
    "    estimated_operator_time = torch.max(\n",
    "        batch_total_flops / (batch_pi * decay_param),\n",
    "        batch_memory_accesses / (predicted_alpha * beta),\n",
    "    )\n",
    "\n",
    "    # weights = 1 + target_operator_time / max(target_operator_time)  # Higher weights for larger operator times\n",
    "    # loss = weighted_mse_loss(target_operator_time, estimated_operator_time, weights)\n",
    "\n",
    "    # log_target_operator_time = torch.log1p(target_operator_time)\n",
    "    # loss = torch.nn.functional.mse_loss(torch.log1p(estimated_operator_time), log_target_operator_time)\n",
    "\n",
    "    # loss = mape_loss(target_operator_time, estimated_operator_time)\n",
    "    loss = torch.nn.functional.mse_loss(estimated_operator_time, target_operator_time)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Define the neural network model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_p=0.2):\n",
    "        super(Net, self).__init__()\n",
    "        layers = []\n",
    "        in_size = input_size\n",
    "\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=dropout_p))  # Add Dropout layer\n",
    "            in_size = hidden_size\n",
    "\n",
    "        layers.append(nn.Linear(in_size, output_size))\n",
    "        layers.append(nn.Sigmoid())  # Ensure output is between 0 and 1\n",
    "        # layers.append(nn.ReLU())\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x).clamp(min=1e-2)\n",
    "\n",
    "\n",
    "# Define model parameters\n",
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_sizes = [256, 128, 64, 64, 32]\n",
    "output_size = 1  # Predicting alpha\n",
    "\n",
    "# Initialize the model\n",
    "model = Net(input_size, hidden_sizes, output_size, dropout_p=0.0)\n",
    "\n",
    "# Define optimizer without weight decay\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=0.0005, weight_decay=0.0001\n",
    ")  # No weight_decay\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 20\n",
    "\n",
    "# For early stopping\n",
    "patience = 10\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Training loop with validation\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # min_predicted_alpha = float(\"inf\")\n",
    "    # max_predicted_alpha = float(\"-inf\")\n",
    "\n",
    "    for (\n",
    "        batch_X,\n",
    "        batch_y,\n",
    "        batch_memory_accesses,\n",
    "        batch_intensity,\n",
    "        batch_pi,\n",
    "    ) in train_loader:\n",
    "        # Forward pass: predict alpha\n",
    "        predicted_alpha = model(batch_X).squeeze()  # .squeeze() to align shapes\n",
    "        min_predicted_alpha = min(min_predicted_alpha, predicted_alpha.min().item())\n",
    "        max_predicted_alpha = max(max_predicted_alpha, predicted_alpha.max().item())\n",
    "\n",
    "        # Compute loss\n",
    "        loss = custom_loss_function(\n",
    "            predicted_alpha,\n",
    "            batch_y,\n",
    "            batch_memory_accesses,\n",
    "            batch_intensity,\n",
    "            batch_pi,\n",
    "            beta,\n",
    "        )\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Average training loss\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (\n",
    "            batch_X_val,\n",
    "            batch_y_val,\n",
    "            batch_memory_accesses_val,\n",
    "            batch_intensity_val,\n",
    "            batch_pi_val,\n",
    "        ) in val_loader:\n",
    "            # Forward pass\n",
    "            predicted_alpha_val = model(batch_X_val).squeeze()\n",
    "\n",
    "            # Compute loss\n",
    "            loss_val = custom_loss_function(\n",
    "                predicted_alpha_val,\n",
    "                batch_y_val,\n",
    "                batch_memory_accesses_val,\n",
    "                batch_intensity_val,\n",
    "                batch_pi_val,\n",
    "                beta,\n",
    "            )\n",
    "\n",
    "            val_loss += loss_val.item()\n",
    "\n",
    "    # Average validation loss\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Min predicted_alpha: {min_predicted_alpha}, Max predicted_alpha: {max_predicted_alpha}\"\n",
    "    )\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")  # Save the best model\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "\n",
    "# Evaluate on validation set\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for (\n",
    "        batch_X_val,\n",
    "        batch_y_val,\n",
    "        batch_memory_accesses_val,\n",
    "        batch_intensity_val,\n",
    "        batch_pi_val,\n",
    "    ) in val_loader:\n",
    "        # Forward pass\n",
    "        predicted_alpha_val = model(batch_X_val).squeeze()\n",
    "\n",
    "        # Compute loss\n",
    "        loss_val = custom_loss_function(\n",
    "            predicted_alpha_val,\n",
    "            batch_y_val,\n",
    "            batch_memory_accesses_val,\n",
    "            batch_intensity_val,\n",
    "            batch_pi_val,\n",
    "            beta,\n",
    "        )\n",
    "\n",
    "        val_loss += loss_val.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Final Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Get predictions and actual values for the validation set\n",
    "all_predicted_times = []\n",
    "all_actual_times = []\n",
    "with torch.no_grad():\n",
    "\n",
    "    for (\n",
    "        batch_X_val,\n",
    "        batch_y_val,\n",
    "        batch_memory_accesses_val,\n",
    "        batch_intensity_val,\n",
    "        batch_pi_val,\n",
    "    ) in val_loader:\n",
    "        # Forward pass: Predict alpha\n",
    "        predicted_alpha_val = model(batch_X_val).squeeze()\n",
    "\n",
    "        # Compute total FLOPs for this batch\n",
    "        batch_total_flops_val = (\n",
    "            batch_intensity_val * batch_memory_accesses_val\n",
    "        )  # GFLOPs\n",
    "\n",
    "        # Ensure no zero values for predicted alpha\n",
    "        predicted_alpha_val = predicted_alpha_val.clamp(min=1e-6)\n",
    "\n",
    "        # Calculate estimated operator time\n",
    "        estimated_operator_time_val = torch.max(\n",
    "            batch_total_flops_val / (batch_pi_val * 0.95),\n",
    "            batch_memory_accesses_val / (predicted_alpha_val * beta),\n",
    "        )\n",
    "\n",
    "        # Inverse log transformation to get the original scale\n",
    "        # estimated_operator_time_val = torch.expm1(estimated_operator_time_val)\n",
    "\n",
    "        # Store predictions and actual values\n",
    "        all_predicted_times.append(estimated_operator_time_val.numpy())\n",
    "        all_actual_times.append(batch_y_val.numpy())\n",
    "\n",
    "# Concatenate all batches\n",
    "all_predicted_times = np.concatenate(all_predicted_times)\n",
    "all_actual_times = np.concatenate(all_actual_times)\n",
    "\n",
    "# Plot actual vs. predicted operator times\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(all_actual_times, all_predicted_times, alpha=0.5)\n",
    "plt.xlabel(\"Actual Operator Time\")\n",
    "plt.ylabel(\"Predicted Operator Time\")\n",
    "plt.title(\"Actual vs. Predicted Operator Times on Validation Set\")\n",
    "plt.plot(\n",
    "    [all_actual_times.min(), all_actual_times.max()],\n",
    "    [all_actual_times.min(), all_actual_times.max()],\n",
    "    \"r--\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict runtime directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Loss: 4.0058, Validation Loss: 3.7969\n",
      "Epoch [2/20], Training Loss: 3.7640, Validation Loss: 3.7911\n",
      "Epoch [3/20], Training Loss: 3.7648, Validation Loss: 3.7952\n",
      "Epoch [4/20], Training Loss: 3.7648, Validation Loss: 3.7925\n",
      "Epoch [5/20], Training Loss: 3.7652, Validation Loss: 3.7923\n",
      "Epoch [6/20], Training Loss: 3.7646, Validation Loss: 3.7952\n",
      "Epoch [7/20], Training Loss: 3.7644, Validation Loss: 3.7956\n",
      "Epoch [8/20], Training Loss: 3.7651, Validation Loss: 3.7932\n",
      "Epoch [9/20], Training Loss: 3.7651, Validation Loss: 3.7973\n",
      "Epoch [10/20], Training Loss: 3.7637, Validation Loss: 3.7912\n",
      "Epoch [11/20], Training Loss: 3.7651, Validation Loss: 3.7926\n",
      "Epoch [12/20], Training Loss: 3.7642, Validation Loss: 3.7933\n",
      "Early stopping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c8/q9xddvxj5pv262qffnm05sk00000gn/T/ipykernel_66573/327155908.py:158: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Loss: 529587.5494\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply log transformation to target (operator times)\n",
    "y_array_log = np.log1p(y_array)  # log(1 + y) to avoid issues with zero values\n",
    "\n",
    "# Split data into training and validation sets, including additional features\n",
    "(\n",
    "    X_train,\n",
    "    X_val,\n",
    "    y_train,\n",
    "    y_val,\n",
    "    memory_accesses_train,\n",
    "    memory_accesses_val,\n",
    "    intensity_train,\n",
    "    intensity_val,\n",
    "    pi_train,\n",
    "    pi_val,\n",
    ") = train_test_split(\n",
    "    X_array,\n",
    "    y_array_log,  # Use log-transformed targets\n",
    "    memory_accesses_array,\n",
    "    intensity_array,\n",
    "    pi_array,\n",
    "    test_size=0.2,\n",
    "    random_state=random_seed,\n",
    ")\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "memory_accesses_train_tensor = torch.tensor(memory_accesses_train, dtype=torch.float32)\n",
    "memory_accesses_val_tensor = torch.tensor(memory_accesses_val, dtype=torch.float32)\n",
    "\n",
    "intensity_train_tensor = torch.tensor(intensity_train, dtype=torch.float32)\n",
    "intensity_val_tensor = torch.tensor(intensity_val, dtype=torch.float32)\n",
    "\n",
    "pi_train_tensor = torch.tensor(pi_train, dtype=torch.float32)\n",
    "pi_val_tensor = torch.tensor(pi_val, dtype=torch.float32)\n",
    "\n",
    "# Create TensorDatasets for training and validation sets\n",
    "train_dataset = TensorDataset(\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    memory_accesses_train_tensor,\n",
    "    intensity_train_tensor,\n",
    "    pi_train_tensor,\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    X_val_tensor,\n",
    "    y_val_tensor,\n",
    "    memory_accesses_val_tensor,\n",
    "    intensity_val_tensor,\n",
    "    pi_val_tensor,\n",
    ")\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 1024\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the neural network model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_p=0.2):\n",
    "        super(Net, self).__init__()\n",
    "        layers = []\n",
    "        in_size = input_size\n",
    "\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=dropout_p))  # Add Dropout layer\n",
    "            in_size = hidden_size\n",
    "\n",
    "        layers.append(nn.Linear(in_size, output_size))\n",
    "        # No activation in the final layer because we are predicting continuous values\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Define model parameters\n",
    "input_size = X_train_tensor.shape[1]\n",
    "hidden_sizes = [256, 128, 64, 64, 32]\n",
    "output_size = 1  # Predicting log-transformed operator times\n",
    "\n",
    "# Initialize the model\n",
    "model = Net(input_size, hidden_sizes, output_size, dropout_p=0.0)\n",
    "\n",
    "# Define optimizer without weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.0001)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 20\n",
    "\n",
    "# For early stopping\n",
    "patience = 10\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Training loop with validation\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_X, batch_y, _, _, _ in train_loader:\n",
    "        # Forward pass: predict log-transformed operator times\n",
    "        predicted_time_log = model(batch_X).squeeze()\n",
    "\n",
    "        # Compute loss using standard MSE\n",
    "        loss = torch.nn.functional.mse_loss(predicted_time_log, batch_y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Average training loss\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_X_val, batch_y_val, _, _, _ in val_loader:\n",
    "            # Forward pass: predict log-transformed operator times\n",
    "            predicted_time_log_val = model(batch_X_val).squeeze()\n",
    "\n",
    "            # Compute validation loss using standard MSE\n",
    "            loss_val = torch.nn.functional.mse_loss(predicted_time_log_val, batch_y_val)\n",
    "\n",
    "            val_loss += loss_val.item()\n",
    "\n",
    "    # Average validation loss\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\"\n",
    "    )\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")  # Save the best model\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "\n",
    "# Evaluate on validation set\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "all_predicted_times = []\n",
    "all_actual_times = []\n",
    "with torch.no_grad():\n",
    "    for batch_X_val, batch_y_val, _, _, _ in val_loader:\n",
    "        # Forward pass: predict log-transformed operator times\n",
    "        predicted_time_log_val = model(batch_X_val).squeeze()\n",
    "\n",
    "        # Apply inverse log transformation to get back the original operator times\n",
    "        predicted_time_val = torch.expm1(predicted_time_log_val)\n",
    "\n",
    "        val_loss += torch.nn.functional.mse_loss(predicted_time_val, torch.expm1(batch_y_val)).item()\n",
    "\n",
    "        # Store predictions and actual values\n",
    "        all_predicted_times.append(predicted_time_val.numpy())\n",
    "        all_actual_times.append(torch.expm1(batch_y_val).numpy())  # Convert log target to original scale\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Final Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Concatenate all batches\n",
    "all_predicted_times = np.concatenate(all_predicted_times)\n",
    "all_actual_times = np.concatenate(all_actual_times)\n",
    "\n",
    "# Plot actual vs. predicted operator times\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(all_actual_times, all_predicted_times, alpha=0.5)\n",
    "plt.xlabel(\"Actual Operator Time\")\n",
    "plt.ylabel(\"Predicted Operator Time\")\n",
    "plt.title(\"Actual vs. Predicted Operator Times on Validation Set\")\n",
    "plt.plot(\n",
    "    [all_actual_times.min(), all_actual_times.max()],\n",
    "    [all_actual_times.min(), all_actual_times.max()],\n",
    "    \"r--\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "class PredictionNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size):\n",
    "        super(PredictionNetwork, self).__init__()\n",
    "        layers = []\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_size = hidden_size\n",
    "        layers.append(nn.Dropout(p=0.5))  # Dropout before the output layer\n",
    "        layers.append(nn.Linear(hidden_layers[-1], output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle_loss(y_pred, y_actual):\n",
    "    return torch.sqrt(torch.mean(torch.square(torch.log1p(y_pred) - torch.log1p(y_actual))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=300):\n",
    "    criterion = rmsle_loss\n",
    "    optimizer = Adam(model.parameters(), lr=0.1, weight_decay=1e-5)  # L2 regularization\n",
    "    scheduler = StepLR(optimizer, step_size=40, gamma=0.5)  # Learning rate decay\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for data, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "        # Optionally validate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            validation_loss = 0\n",
    "            for data, targets in val_loader:\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "                validation_loss += loss.item()\n",
    "            print(f'Validation Loss: {validation_loss / len(val_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your data loaders\n",
    "# train_loader, val_loader = setup_data_loaders()\n",
    "\n",
    "# Assuming a certain input size and output size\n",
    "model = PredictionNetwork(input_size=10, hidden_layers=[64, 128], output_size=1)\n",
    "# train_model(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95155,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train_scaled.shape\n",
    "# X_val_scaled.shape\n",
    "# X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    y_train_t = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "    X_val_t = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "    y_val_t = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating datasets\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "val_dataset = TensorDataset(X_val_t, y_val_t)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network Architecture\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train_t.shape[1], 20)\n",
    "        self.fc2 = nn.Linear(50, 20)\n",
    "        self.fc3 = nn.Linear(20, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class EnhancedRegressionNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnhancedRegressionNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train_t.shape[1], 100)  # Increase the number of neurons\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, 20)\n",
    "        self.fc4 = nn.Linear(20, 10)\n",
    "        self.fc5 = nn.Linear(10, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.3)  # Adding dropout for regularization\n",
    "\n",
    "        # Optional: Include batch normalization layers\n",
    "        self.batch_norm1 = nn.BatchNorm1d(100)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(50)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(20)\n",
    "        self.batch_norm4 = nn.BatchNorm1d(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.batch_norm1(self.fc1(x)))\n",
    "        x = self.dropout(x)  # Applying dropout\n",
    "        x = self.relu(self.batch_norm2(self.fc2(x)))\n",
    "        x = self.relu(self.batch_norm3(self.fc3(x)))\n",
    "        x = self.relu(self.batch_norm4(self.fc4(x)))\n",
    "        x = self.fc5(x)  # Output layer without\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 634369 entries, 0 to 684936\n",
      "Data columns (total 16 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   b             634369 non-null  int64  \n",
      " 1   in_channels   634369 non-null  int64  \n",
      " 2   iH            634369 non-null  int64  \n",
      " 3   iW            634369 non-null  int64  \n",
      " 4   out_channels  634369 non-null  int64  \n",
      " 5   groups        634369 non-null  int64  \n",
      " 6   kH            634369 non-null  int64  \n",
      " 7   kW            634369 non-null  int64  \n",
      " 8   stride        634369 non-null  int64  \n",
      " 9   dilation      634369 non-null  int64  \n",
      " 10  gflops        634369 non-null  float64\n",
      " 11  dtype_16      634369 non-null  bool   \n",
      " 12  dtype_32      634369 non-null  bool   \n",
      " 13  dtype_b16     634369 non-null  bool   \n",
      " 14  transposed_0  634369 non-null  bool   \n",
      " 15  transposed_1  634369 non-null  bool   \n",
      "dtypes: bool(5), float64(1), int64(10)\n",
      "memory usage: 61.1 MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 694344.6250, Validation Loss: 500541.3714\n",
      "Epoch 2, Train Loss: 350.9424, Validation Loss: 493101.3078\n",
      "Epoch 3, Train Loss: 95348.5391, Validation Loss: 482951.1448\n",
      "Epoch 4, Train Loss: 155022.1875, Validation Loss: 474194.9193\n",
      "Epoch 5, Train Loss: 734.2679, Validation Loss: 457000.2978\n",
      "Epoch 6, Train Loss: 17726.1074, Validation Loss: 418684.1120\n",
      "Epoch 7, Train Loss: 1518.5846, Validation Loss: 397694.7514\n",
      "Epoch 8, Train Loss: 7547.5752, Validation Loss: 390082.7111\n",
      "Epoch 9, Train Loss: 4378.8276, Validation Loss: 360240.3024\n",
      "Epoch 10, Train Loss: 6034.9473, Validation Loss: 315839.8117\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     25\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 26\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Validation loop\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/research/lib/python3.10/site-packages/torch/optim/optimizer.py:479\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    475\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    476\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m             )\n\u001b[0;32m--> 479\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    482\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/research/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/research/lib/python3.10/site-packages/torch/optim/adam.py:231\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    219\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    221\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    222\u001b[0m         group,\n\u001b[1;32m    223\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m         state_steps,\n\u001b[1;32m    229\u001b[0m     )\n\u001b[0;32m--> 231\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/miniconda3/envs/research/lib/python3.10/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/research/lib/python3.10/site-packages/torch/optim/adam.py:776\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    774\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 776\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/research/lib/python3.10/site-packages/torch/optim/adam.py:438\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    436\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 438\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    440\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO: write a script that\n",
    "1) Takes a set of learning rates\n",
    "2) Takes a potential model name\n",
    "3) outputs a bunch of validation losses in a new folder.\n",
    "\"\"\"\n",
    "lr = 1e-4\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = EnhancedRegressionNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "validation_losses = []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss = loss.item()\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, targets.view(-1, 1)).item()\n",
    "        val_loss /= len(val_loader)\n",
    "    validation_losses.append(val_loss)\n",
    "    # if (epoch + 1) % 2 == 0:\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 7.3877e-01,  3.1969e-01,  5.8624e-01,  6.3714e-01,  3.1101e-02,\n",
      "         -3.1253e+00,  2.5411e-01,  4.2812e-01,  2.1393e-37,  2.2994e-37,\n",
      "          7.5621e-03, -5.7961e-01,  1.4561e+00, -1.4630e+00,  1.1861e-01,\n",
      "         -1.2178e-01],\n",
      "        [ 9.1689e-01,  9.7033e-01,  7.0651e-01,  1.0657e+00,  1.9568e-01,\n",
      "         -1.5035e+00,  5.1491e-01,  4.2059e-01,  1.7083e-37,  2.7001e-37,\n",
      "          1.4968e-02, -2.9886e-01,  1.8517e+00, -1.8406e+00, -5.3701e-01,\n",
      "          8.5764e-01],\n",
      "        [ 3.8090e-03,  5.0351e-03,  2.1848e-03,  2.4860e-03,  4.2933e-03,\n",
      "         -2.7401e+00,  2.3639e-03, -8.9123e-04,  2.0427e-37, -4.2463e-38,\n",
      "          4.2975e-01,  1.4509e-01,  1.4507e-01,  1.4317e-01,  3.5634e-02,\n",
      "          3.7120e-02],\n",
      "        [ 6.2758e-01,  6.8996e-01,  8.1576e-01,  7.0331e-01,  8.7013e-02,\n",
      "         -8.9521e-01,  2.6054e-01,  2.2520e-01,  5.6746e-37, -2.0159e-37,\n",
      "          4.3432e-01, -2.3801e-01,  9.1401e-01, -1.2021e+00, -7.4300e-01,\n",
      "          4.3373e-01],\n",
      "        [ 5.1709e-01,  2.3051e-01,  3.1979e-01,  4.8097e-01,  1.0441e-01,\n",
      "         -3.7867e+00,  3.5079e-01,  6.4978e-01,  2.2564e-37, -8.3048e-38,\n",
      "          4.5706e-03, -8.0745e-01,  1.7686e+00, -1.4246e+00, -6.8049e-02,\n",
      "         -3.7269e-01],\n",
      "        [ 4.9144e-01,  7.8607e-01,  4.5194e-01,  4.8205e-01, -1.0544e-01,\n",
      "         -4.1970e+00,  5.5887e-01,  4.4517e-01, -4.3498e-37,  2.3073e-37,\n",
      "         -6.5340e-02, -1.2725e+00,  1.9956e+00, -1.1539e+00,  4.0537e-01,\n",
      "         -4.6714e-01],\n",
      "        [ 7.8299e-03,  2.9731e-02,  1.4406e-02,  1.2801e-02,  3.6541e-02,\n",
      "         -3.3198e+00,  8.9921e-03,  3.0470e-03, -2.1579e-37,  2.2775e-37,\n",
      "          4.9008e-01,  4.4065e-02,  8.7554e-02,  4.1357e-02, -1.8965e-01,\n",
      "         -1.8597e-01],\n",
      "        [ 7.5459e-01,  5.7367e-01,  6.9390e-01,  1.0904e+00,  3.1082e-01,\n",
      "         -1.2388e+00,  9.2192e-01,  4.3690e-01, -4.6063e-37,  1.1617e-37,\n",
      "          1.7080e-01, -2.8642e-01,  1.6382e+00, -1.0998e+00, -7.9537e-01,\n",
      "          3.5216e-01],\n",
      "        [ 4.2761e-01,  5.1359e-01,  5.7677e-01,  6.8281e-01, -7.2674e-02,\n",
      "         -3.4870e-01,  2.6098e-01,  2.1522e-01,  2.0902e-37,  3.3918e-37,\n",
      "          8.4762e-01, -8.0894e-01,  4.3734e-01, -8.2825e-02, -4.2433e-01,\n",
      "          5.6640e-01],\n",
      "        [ 8.9166e-03,  4.2526e-02,  1.7016e-02,  1.2412e-02,  6.8253e-02,\n",
      "         -2.7165e+00,  1.5053e-02,  5.5756e-03, -1.5904e-37, -4.2018e-37,\n",
      "          4.0197e-01, -1.8182e-01,  1.4362e-02, -1.0155e-01, -1.3305e-01,\n",
      "         -1.8077e-01],\n",
      "        [ 6.5836e-01,  4.0126e-02,  6.9941e-02,  1.0980e-01,  4.2820e-02,\n",
      "         -7.6473e-01, -7.2722e-02,  1.6648e-01,  1.2448e-37, -2.2914e-37,\n",
      "          7.0005e-01, -1.1134e-01,  4.9070e-01, -6.4026e-01, -6.9023e-01,\n",
      "          3.0797e-01],\n",
      "        [ 1.0252e+00,  9.2707e-01,  9.4823e-01,  7.3006e-01,  5.1374e-01,\n",
      "         -1.7904e+00,  5.8520e-01,  3.2102e-01, -2.2935e-37, -2.4086e-37,\n",
      "         -1.0217e-01, -5.5806e-01,  1.3579e+00, -1.1799e+00, -7.2658e-01,\n",
      "          9.1930e-01],\n",
      "        [ 8.4399e-01,  2.1868e-01,  2.7131e-01,  7.7202e-01,  1.2193e-01,\n",
      "         -1.0730e+00,  4.4170e-01,  5.7172e-01,  1.9515e-37, -3.9268e-37,\n",
      "          4.2564e-01,  9.9961e-02,  1.4951e+00, -1.7117e+00, -5.7062e-01,\n",
      "          4.1307e-01],\n",
      "        [ 8.8371e-01,  6.0344e-01,  4.0559e-01,  8.9800e-01, -2.9213e-01,\n",
      "         -1.9122e+00,  6.9197e-01,  2.5283e-01, -1.9018e-37, -8.7931e-38,\n",
      "          1.1053e-01, -4.9006e-01,  1.5053e+00, -1.5436e+00, -3.9466e-01,\n",
      "          7.9852e-01],\n",
      "        [ 7.5838e-01,  2.8403e-01,  4.0522e-01,  6.0064e-01, -3.8730e-02,\n",
      "         -1.0997e+00,  7.0593e-01,  5.5704e-01, -1.8573e-37,  2.0256e-37,\n",
      "          7.0293e-01,  5.9709e-02,  3.7586e-01, -8.4603e-01, -2.7730e-01,\n",
      "          3.7000e-01],\n",
      "        [ 5.0289e-01,  3.7407e-01,  2.9555e-01,  4.1422e-01,  1.3277e-01,\n",
      "         -1.0098e+00,  4.2870e-01,  2.8082e-01,  2.0939e-37, -2.1901e-37,\n",
      "          4.0947e-01,  7.8106e-02,  1.0892e+00, -1.8205e+00, -4.2782e-01,\n",
      "          1.0470e-01],\n",
      "        [ 6.6947e-03,  2.5542e-02,  1.0469e-02,  8.0997e-03,  3.1959e-02,\n",
      "         -2.7499e+00,  9.1177e-03,  2.0367e-03,  2.1253e-37, -2.0845e-38,\n",
      "          4.1867e-01, -9.0092e-02,  6.6618e-02,  2.0190e-02, -1.5278e-01,\n",
      "         -1.6847e-01],\n",
      "        [ 8.3835e-01,  6.6933e-01,  9.3156e-01,  1.1210e+00,  2.4551e-01,\n",
      "         -4.5031e-01,  1.7424e-01,  1.7650e-01,  2.3666e-37,  2.9044e-37,\n",
      "          4.1955e-01, -2.0576e-01,  8.2421e-01, -8.6273e-01, -3.5688e-01,\n",
      "          3.1571e-01],\n",
      "        [ 5.8199e-02,  3.2816e-01,  1.6469e-01,  1.6811e-01,  1.5052e-01,\n",
      "         -3.1545e+00,  1.1578e-01,  9.2304e-02, -2.0072e-37,  4.1214e-38,\n",
      "          1.9779e-01,  1.3349e-01,  2.7243e-01,  1.9696e-01,  1.3073e-01,\n",
      "          5.0948e-03],\n",
      "        [ 5.0720e-01,  8.3036e-01,  7.1454e-01,  9.0781e-01, -7.5129e-02,\n",
      "         -3.8618e-01,  5.3241e-01,  4.2656e-01, -2.4192e-37,  1.5976e-37,\n",
      "          5.8713e-01, -1.9197e-01,  6.8722e-01, -1.0643e+00, -1.4151e-01,\n",
      "          1.7772e-01],\n",
      "        [ 4.6161e-01,  2.6707e-01,  6.3590e-02,  1.1164e-01,  9.6465e-02,\n",
      "         -3.4354e+00,  1.6452e-01,  3.2794e-01, -2.3577e-37, -2.3941e-37,\n",
      "          5.7067e-02, -7.6436e-01,  1.2863e+00, -7.8560e-01,  2.7169e-01,\n",
      "         -2.5498e-01],\n",
      "        [ 1.5124e-01,  8.3422e-02, -1.8801e-01, -1.3720e-01,  6.6119e-01,\n",
      "         -4.2010e+00,  5.4603e-01,  3.2411e-01, -1.3331e-37, -3.0303e-37,\n",
      "          8.6830e-02, -8.7039e-01,  1.1955e+00, -9.1607e-01,  2.8106e-01,\n",
      "         -5.4656e-01],\n",
      "        [ 5.2953e-01,  6.1870e-01,  5.9012e-01,  8.3953e-01,  2.2161e-01,\n",
      "         -1.0703e+00,  6.7953e-01,  3.5109e-01,  2.2209e-37, -4.1520e-37,\n",
      "          3.8961e-01, -8.1840e-01,  1.0409e+00, -5.4436e-01, -5.8449e-01,\n",
      "          6.3919e-01],\n",
      "        [ 9.1258e-01,  4.5215e-01,  5.9440e-01,  8.1463e-01,  8.4563e-02,\n",
      "         -2.5472e+00,  4.5006e-01,  6.5627e-01, -2.1532e-37, -2.1027e-37,\n",
      "         -5.2997e-02, -1.0337e+00,  2.0632e+00, -1.2549e+00,  9.4586e-02,\n",
      "          5.8671e-02],\n",
      "        [ 8.5662e-03,  5.7749e-02,  1.9095e-02,  1.1841e-02,  9.1940e-02,\n",
      "         -3.0619e+00,  1.7703e-02,  8.6362e-03, -8.2625e-38, -2.1599e-37,\n",
      "          4.7300e-01, -1.6325e-01,  1.3046e-01, -4.0005e-02,  2.7035e-01,\n",
      "          1.7046e-01],\n",
      "        [ 5.2319e-03,  3.4586e-02,  1.1793e-02,  8.5899e-03,  5.4614e-02,\n",
      "         -3.0749e+00,  1.0475e-02,  2.2217e-03, -2.1267e-37, -2.1707e-37,\n",
      "          4.5842e-01,  1.2130e-01,  2.3614e-01,  1.3825e-01,  2.6277e-02,\n",
      "         -1.5270e-03],\n",
      "        [ 1.3925e-03,  1.1996e-03, -3.1393e-05,  5.8541e-04,  1.5926e-04,\n",
      "         -2.8975e+00,  4.3569e-04, -1.1632e-03,  2.3174e-37,  2.2055e-37,\n",
      "          4.7309e-01,  5.0010e-02,  4.9218e-02,  4.8121e-02,  4.9871e-02,\n",
      "          4.7808e-02],\n",
      "        [ 5.4957e-01,  8.5861e-01,  6.0664e-01,  8.5166e-01,  2.3224e-01,\n",
      "         -8.7898e-01,  4.0039e-01,  2.7989e-01,  2.3025e-37, -2.4441e-37,\n",
      "          3.4306e-01, -5.0722e-01,  1.5585e+00, -9.3288e-01, -6.7829e-01,\n",
      "          7.5000e-01],\n",
      "        [ 7.2812e-01,  5.3403e-01,  5.1131e-01,  7.3751e-01,  5.3444e-02,\n",
      "         -1.3450e+00,  8.3643e-01,  7.2233e-01,  2.1182e-37,  2.1868e-37,\n",
      "          3.2150e-01, -8.4665e-01,  1.5269e+00, -9.9037e-01,  4.5697e-03,\n",
      "          1.4537e-02],\n",
      "        [ 4.1770e-01,  2.5522e-01, -1.2527e-01,  2.4623e-02,  1.0490e-01,\n",
      "         -2.6714e+00,  3.4729e-01,  3.5644e-01, -2.2417e-37,  1.2959e-37,\n",
      "          1.3208e-01, -4.7529e-01,  1.6139e+00, -9.0137e-01,  2.0722e-02,\n",
      "         -2.8777e-01],\n",
      "        [ 1.0249e-02,  4.1963e-02,  4.7361e-02,  3.6020e-02,  1.3022e-02,\n",
      "         -1.6624e+00,  7.8988e-03, -1.0259e-02, -2.0921e-37,  2.3837e-37,\n",
      "          5.4823e-01, -3.5415e-01,  1.6240e-01,  5.8250e-02, -2.4012e-02,\n",
      "         -1.8209e-01],\n",
      "        [ 2.9851e-01,  2.1590e-01,  5.3730e-01,  5.9138e-01, -3.5673e-01,\n",
      "         -1.4403e-01,  8.2786e-02,  3.9066e-03, -2.0627e-37,  2.3783e-37,\n",
      "          6.5442e-01, -4.5888e-01,  4.9273e-01, -4.9127e-01, -1.5788e-01,\n",
      "          1.6401e-01],\n",
      "        [ 1.9364e-01,  1.9644e-01,  3.4874e-01,  3.8028e-01, -2.5359e-01,\n",
      "         -5.5185e-01, -2.7151e-02, -6.4768e-02,  8.8021e-38, -2.2855e-37,\n",
      "          9.9928e-01,  1.9481e-01,  1.6011e-01, -4.0145e-01, -1.2273e-01,\n",
      "         -4.1418e-02],\n",
      "        [ 1.6108e-02,  6.4417e-02,  2.9553e-02,  2.7143e-02,  7.8468e-02,\n",
      "         -3.2148e+00,  1.7240e-02,  8.0184e-03, -2.3638e-37,  1.6075e-37,\n",
      "          4.1086e-01, -1.4664e-01, -6.2280e-02, -1.5211e-01, -6.2781e-02,\n",
      "         -5.9523e-02],\n",
      "        [ 1.0755e-03, -9.9012e-04, -2.2265e-03, -1.6692e-03, -1.7765e-03,\n",
      "         -2.2459e+00,  1.7807e-03, -3.4282e-04, -1.8677e-37, -2.1337e-37,\n",
      "          5.9552e-01, -6.3671e-02,  2.8149e-01,  2.7932e-03, -2.4492e-01,\n",
      "         -2.5230e-01],\n",
      "        [ 6.9922e-01,  5.6026e-01,  7.5511e-01,  9.1984e-01,  3.0233e-01,\n",
      "         -3.9113e+00,  7.6047e-01,  5.0461e-01, -1.9627e-37, -2.4741e-37,\n",
      "         -1.8041e-01, -6.6551e-01,  1.6634e+00, -9.9128e-01,  4.3899e-01,\n",
      "         -1.0219e-01],\n",
      "        [ 6.2233e-01,  4.3742e-01,  2.9808e-01,  5.3491e-01, -2.2678e-01,\n",
      "         -8.4539e-01,  8.1973e-01,  5.1260e-01, -9.0144e-38,  2.3149e-37,\n",
      "          4.7009e-01, -5.7567e-02,  1.0976e+00, -1.3786e+00, -8.7087e-01,\n",
      "          5.5785e-01],\n",
      "        [ 5.3193e-01,  2.1530e-01,  6.7171e-01,  8.5911e-01,  4.6380e-02,\n",
      "         -2.9638e-01,  2.4207e-01,  2.0374e-01, -3.2922e-38,  2.0892e-37,\n",
      "          9.2675e-01, -1.1466e-03, -1.2455e-02, -5.9608e-02,  5.3658e-02,\n",
      "         -6.6695e-02],\n",
      "        [ 3.5110e-01,  4.1099e-01,  4.1445e-01,  5.7872e-01, -8.7097e-02,\n",
      "         -1.3121e+00,  7.1215e-01,  3.2707e-02,  2.2138e-37, -2.2670e-37,\n",
      "          3.8200e-01, -2.7535e-01,  9.6586e-01, -9.1894e-01, -6.2050e-02,\n",
      "          1.5687e-01],\n",
      "        [ 4.3192e-01,  6.0332e-01,  6.7297e-01,  9.2857e-01, -2.7438e-01,\n",
      "         -6.5744e-01,  3.7302e-01,  1.8403e-01,  3.2704e-37, -2.7626e-37,\n",
      "          5.3581e-01, -3.2379e-02,  4.2973e-01, -5.8737e-01, -4.7661e-01,\n",
      "          2.8849e-01],\n",
      "        [-2.1813e-01,  8.3213e-01,  5.3600e-01,  5.2963e-01,  2.0221e-01,\n",
      "         -3.9062e+00,  3.1965e-01,  4.0960e-01,  2.0772e-37, -5.8030e-38,\n",
      "          5.2627e-02, -1.0721e+00,  1.5221e+00, -7.0572e-01,  4.7287e-01,\n",
      "         -7.2600e-01],\n",
      "        [ 7.3804e-01,  4.8238e-01,  6.0269e-01,  5.0655e-01,  1.4478e-02,\n",
      "         -5.4613e-01,  3.1863e-01,  2.7878e-02,  2.0362e-37,  1.0222e-37,\n",
      "          7.9994e-01, -3.5585e-01,  6.2344e-01, -2.7705e-01, -4.1426e-01,\n",
      "          2.4486e-01],\n",
      "        [ 2.3233e-01,  4.8498e-01,  3.5524e-01,  4.9012e-01,  6.3676e-02,\n",
      "         -5.9332e-01,  3.6435e-01,  2.8085e-02, -2.0984e-37,  1.4430e-37,\n",
      "          8.6684e-01, -8.1330e-02,  3.8079e-01, -7.2736e-01, -3.0351e-01,\n",
      "          4.5931e-01],\n",
      "        [ 7.7034e-01,  1.0600e+00,  9.2763e-01,  1.2964e+00,  2.3519e-01,\n",
      "         -2.4314e+00,  8.9009e-01,  6.1663e-01,  2.3072e-37, -2.1073e-37,\n",
      "         -2.8003e-01, -3.9992e-01,  8.7990e-01, -9.1802e-01, -1.5588e-01,\n",
      "          2.0654e-01],\n",
      "        [ 5.5061e-01,  4.4694e-01,  9.7714e-01,  7.8616e-01,  1.0598e-01,\n",
      "         -6.7348e-01,  5.2458e-01,  5.3815e-01,  2.3923e-37,  1.5110e-37,\n",
      "          3.6731e-01, -5.1210e-01,  1.0384e+00, -1.0546e+00, -3.6370e-01,\n",
      "          2.8274e-01],\n",
      "        [ 8.9910e-01,  5.6761e-01,  4.6522e-01,  8.8294e-01, -1.4076e-01,\n",
      "         -1.3295e+00,  8.5807e-01,  4.0992e-01, -1.8409e-37,  1.0120e-37,\n",
      "          1.8683e-01, -3.2546e-02,  1.5188e+00, -1.5451e+00, -7.6548e-01,\n",
      "          5.1203e-01],\n",
      "        [ 4.8711e-01,  5.6537e-01,  5.4653e-01,  5.4227e-01, -1.5495e-01,\n",
      "         -5.3549e-01,  1.7211e-01,  2.8605e-01, -1.8109e-37, -1.6460e-37,\n",
      "          4.4752e-01,  1.7437e-01,  9.2800e-01, -1.5023e+00, -1.6427e-01,\n",
      "          1.0561e-01],\n",
      "        [ 3.8626e-02,  8.9913e-02,  5.9994e-02,  5.7781e-02,  1.2673e-01,\n",
      "         -3.3425e+00,  2.7282e-02,  1.5514e-02, -1.3856e-37, -1.8948e-37,\n",
      "          3.6126e-01, -8.4588e-02, -1.5911e-02, -9.9308e-02,  3.2796e-02,\n",
      "          5.4800e-02],\n",
      "        [ 9.1154e-01,  5.6082e-01,  7.1291e-01,  1.0374e+00, -1.3986e-01,\n",
      "         -1.3246e+00,  5.0485e-01,  2.8036e-01,  2.0166e-37,  2.0584e-37,\n",
      "          4.1238e-01, -5.3816e-01,  8.0395e-01, -6.8109e-01, -5.4528e-01,\n",
      "          4.8285e-01],\n",
      "        [ 7.8546e-01,  1.0316e+00,  1.1051e+00,  1.1857e+00,  1.2375e-01,\n",
      "         -1.0774e+00,  4.7493e-01,  4.1569e-01, -3.2807e-37,  2.2485e-37,\n",
      "          1.3171e-01, -5.9929e-01,  1.2528e+00, -9.4606e-01, -5.3005e-01,\n",
      "          8.2143e-01]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 6.7919e-01,  1.1599e+00, -1.6994e+00,  1.1076e+00,  8.5101e-01,\n",
      "         8.5223e-01, -2.1012e+00,  1.0440e+00,  2.5644e-01, -1.8803e+00,\n",
      "        -5.7315e-01,  1.2784e+00,  6.5163e-01,  1.1472e+00,  5.8901e-01,\n",
      "        -1.1907e-01, -1.8339e+00,  1.0959e+00, -2.0413e+00,  1.3202e+00,\n",
      "        -3.3921e-02, -4.2208e-01,  1.4460e+00,  9.4281e-01, -2.1976e+00,\n",
      "        -2.0284e+00, -1.7942e+00,  1.4546e+00,  1.0891e+00, -2.0054e-03,\n",
      "        -1.4845e+00, -7.4392e-02, -6.3643e-01, -2.0752e+00, -1.7842e+00,\n",
      "         1.2098e+00,  8.0109e-01,  5.8165e-01,  4.9574e-01,  6.2867e-01,\n",
      "        -2.9204e-01,  7.1955e-01,  5.0701e-01,  1.1886e+00,  1.0596e+00,\n",
      "         1.3519e+00,  3.2659e-01, -2.1369e+00,  8.5520e-01,  1.6147e+00],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.3997,  1.2009,  0.9888,  0.8578,  1.7028,  1.8513,  1.2166,  1.0569,\n",
      "          0.5722,  0.9357,  0.7775,  1.2856,  1.0511,  1.4046,  0.6060,  1.0123,\n",
      "          0.8551,  0.8826,  1.7906,  0.8311,  1.3465,  1.7941,  0.9014,  1.3347,\n",
      "          1.1115,  1.0314,  0.9235,  0.9322,  0.8680,  1.1638,  0.6043,  0.5832,\n",
      "          0.3916,  1.2527,  0.5579,  1.8686,  1.0060,  0.6329,  0.8042,  0.7102,\n",
      "          1.7692,  0.7558,  0.6718,  1.4960,  0.9716,  1.1386,  0.7617,  1.3704,\n",
      "          1.0943,  1.1426],\n",
      "        [ 0.4693,  0.6097, -2.1236,  0.1043,  0.4507,  0.5309, -2.7987,  0.3240,\n",
      "         -0.1615, -2.8500, -0.4850,  0.7981,  0.0036,  0.5349, -0.1166, -0.1070,\n",
      "         -1.8507,  0.0099, -1.8008,  0.0408,  0.2263, -0.0845,  0.2574,  0.6265,\n",
      "         -2.6408, -2.8355, -2.8561,  0.2923,  0.1385,  0.2107, -1.2078, -0.2490,\n",
      "         -0.8302, -3.1786, -2.2126,  0.8981,  0.1775, -0.5299,  0.1672, -0.0227,\n",
      "         -0.0382, -0.2066, -0.3129,  1.0102,  0.2453,  0.3300, -0.0873, -2.9727,\n",
      "         -0.0109,  0.4098],\n",
      "        [ 1.4716,  1.2911,  1.0906,  0.9354,  1.6703,  1.8047,  1.1409,  1.0917,\n",
      "          0.5692,  1.1289,  0.5969,  1.3187,  0.8157,  1.2853,  0.7806,  0.9661,\n",
      "          0.9971,  0.9247,  1.7245,  0.6911,  1.3660,  1.7445,  1.0838,  1.4035,\n",
      "          0.9931,  0.9624,  0.8027,  0.9163,  0.8961,  1.1443,  0.6288,  0.5940,\n",
      "          0.5344,  1.2022,  0.7457,  1.7386,  0.8454,  0.6173,  0.8170,  0.9066,\n",
      "          1.7721,  0.6738,  0.7545,  1.5846,  1.0007,  1.1144,  0.9063,  1.4830,\n",
      "          1.0974,  1.2772],\n",
      "        [ 0.5089,  0.6675, -2.0329,  0.1503,  0.3537,  0.5174, -2.6406,  0.4378,\n",
      "         -0.2681, -2.9251, -0.1914,  0.5446,  0.2506,  0.5780, -0.0998,  0.1220,\n",
      "         -2.0086,  0.2190, -1.4894, -0.0521,  0.1717,  0.1383,  0.1396,  0.3935,\n",
      "         -2.6245, -2.9335, -3.0001,  0.1495,  0.2829,  0.2291, -1.1125, -0.3795,\n",
      "         -0.7892, -2.9647, -2.0630,  0.7028, -0.0860, -0.3508,  0.1016, -0.0904,\n",
      "          0.0347, -0.1324, -0.4038,  0.8478,  0.0567,  0.2988, -0.1151, -2.8397,\n",
      "          0.2330,  0.4636],\n",
      "        [ 1.3686,  1.3931,  1.1357,  0.9676,  1.5747,  1.8759,  1.0722,  1.0502,\n",
      "          0.7866,  0.9526,  0.7907,  1.4458,  0.9382,  1.2621,  0.6187,  0.9779,\n",
      "          0.8550,  0.8324,  1.9292,  0.8512,  1.5646,  1.6489,  1.1134,  1.3351,\n",
      "          1.0018,  1.0319,  0.8752,  1.0529,  1.0942,  1.1133,  0.7390,  0.7982,\n",
      "          0.5787,  1.4019,  0.7721,  1.8674,  0.9794,  0.7276,  0.7883,  0.7076,\n",
      "          1.8208,  0.7513,  0.7628,  1.4816,  0.9152,  1.2865,  1.0411,  1.4767,\n",
      "          0.9250,  1.2747],\n",
      "        [ 0.4219,  0.6385, -2.1656,  0.1230,  0.4489,  0.4858, -2.5137,  0.3059,\n",
      "         -0.3262, -2.6900, -0.4608,  0.6106,  0.1862,  0.5865, -0.1359, -0.1462,\n",
      "         -1.6940,  0.1811, -1.6837, -0.0862,  0.1064, -0.1128,  0.1604,  0.5377,\n",
      "         -2.4183, -2.8043, -2.7836,  0.1779,  0.3903,  0.1297, -0.9959, -0.4375,\n",
      "         -0.7662, -2.8978, -1.9778,  0.7430,  0.1039, -0.3184,  0.0430,  0.0154,\n",
      "         -0.0317, -0.1764, -0.2257,  0.9240,  0.1735,  0.3312,  0.0560, -2.9764,\n",
      "          0.0450,  0.5225],\n",
      "        [ 0.4589,  0.4782, -2.0765,  0.2278,  0.4390,  0.5135, -2.6482,  0.4701,\n",
      "         -0.3126, -2.7716, -0.1990,  0.7374,  0.0990,  0.4963, -0.0543, -0.0617,\n",
      "         -1.8547,  0.0080, -1.5614,  0.0944,  0.1496, -0.1919,  0.1798,  0.6084,\n",
      "         -2.3955, -2.8732, -2.7812,  0.1885,  0.1749,  0.1807, -1.1747, -0.3216,\n",
      "         -0.8110, -3.0593, -2.0021,  0.7064,  0.0485, -0.4038,  0.1379, -0.0565,\n",
      "         -0.1207, -0.2093, -0.3525,  0.9975,  0.2647,  0.4510, -0.1730, -2.8197,\n",
      "          0.2295,  0.4543],\n",
      "        [ 0.4791,  0.6217, -2.1225,  0.0318,  0.4372,  0.3981, -2.5494,  0.3382,\n",
      "         -0.2525, -2.6494, -0.2637,  0.7021, -0.0179,  0.4616, -0.1853, -0.0542,\n",
      "         -1.7368,  0.0684, -1.5587, -0.1088,  0.2535, -0.3082,  0.2178,  0.5455,\n",
      "         -2.4046, -2.7535, -2.7944,  0.3207,  0.2790,  0.0698, -1.0684, -0.2409,\n",
      "         -0.7893, -2.8732, -1.9760,  0.7967,  0.1304, -0.1950,  0.1226, -0.1068,\n",
      "         -0.0202, -0.1953, -0.1345,  1.0074,  0.2403,  0.3398, -0.0926, -3.0234,\n",
      "          0.2271,  0.4643],\n",
      "        [ 1.3668,  1.0720,  0.8312,  0.9919,  1.4925,  1.8221,  0.9963,  1.1562,\n",
      "          0.5808,  0.9676,  0.7782,  1.3591,  0.9130,  1.3061,  0.8141,  0.7896,\n",
      "          0.8205,  0.8559,  1.6742,  0.7648,  1.5763,  1.6926,  1.0285,  1.3648,\n",
      "          1.0218,  1.0771,  0.9120,  0.8839,  0.9852,  1.1560,  0.6271,  0.5546,\n",
      "          0.5993,  1.3843,  0.7157,  1.8453,  0.9659,  0.7139,  0.9231,  0.8330,\n",
      "          1.7338,  0.7389,  0.5114,  1.6117,  1.0660,  1.1636,  1.0100,  1.4754,\n",
      "          1.1268,  1.2123],\n",
      "        [ 1.3667,  1.3490,  0.9263,  0.9666,  1.5588,  2.0081,  1.0768,  1.1455,\n",
      "          0.8349,  0.9244,  0.6162,  1.2466,  1.0626,  1.2716,  0.8519,  0.8359,\n",
      "          0.8479,  1.0508,  1.7604,  0.8145,  1.3913,  1.8454,  0.9012,  1.3820,\n",
      "          1.1263,  1.1029,  0.9980,  0.9445,  1.0665,  1.1648,  0.6801,  0.6033,\n",
      "          0.3648,  1.2178,  0.7067,  1.7583,  0.8853,  0.5810,  0.9018,  0.8122,\n",
      "          1.8359,  0.6450,  0.6343,  1.5326,  1.1407,  1.2393,  0.9875,  1.6258,\n",
      "          1.0140,  1.1200],\n",
      "        [ 1.3808,  1.3452,  0.9518,  0.9107,  1.5984,  1.8329,  1.0511,  1.1632,\n",
      "          0.6210,  1.0416,  0.7798,  1.4340,  1.0095,  1.4886,  0.8348,  0.8923,\n",
      "          0.9439,  0.7891,  1.7952,  0.7611,  1.5230,  1.7168,  1.0158,  1.3537,\n",
      "          1.1058,  1.0519,  0.9013,  1.0849,  1.1439,  1.3156,  0.5152,  0.7885,\n",
      "          0.4418,  1.2697,  0.6004,  1.7898,  1.0375,  0.6554,  0.8839,  0.9019,\n",
      "          1.9838,  0.6829,  0.7091,  1.6078,  0.9408,  1.0256,  0.8039,  1.6013,\n",
      "          1.0809,  1.2236],\n",
      "        [ 1.3389,  1.3492,  1.0274,  0.7992,  1.4826,  1.9423,  1.0667,  1.0143,\n",
      "          0.8055,  0.9247,  0.6016,  1.2253,  0.9316,  1.2278,  0.6283,  0.9212,\n",
      "          0.8969,  0.9841,  1.7023,  0.9363,  1.4768,  1.6089,  1.0864,  1.5361,\n",
      "          1.1604,  0.9798,  0.9741,  0.9958,  0.9351,  1.0972,  0.5383,  0.7739,\n",
      "          0.5192,  1.4229,  0.7503,  1.8669,  1.0348,  0.5925,  0.8916,  0.9263,\n",
      "          1.9644,  0.6411,  0.6343,  1.5848,  0.8655,  1.1818,  0.8793,  1.4530,\n",
      "          0.9464,  1.2262],\n",
      "        [ 1.5366,  1.1967,  1.1098,  0.8639,  1.6153,  1.7524,  1.1198,  1.1297,\n",
      "          0.7748,  1.0113,  0.6804,  1.3027,  0.9974,  1.4765,  0.6600,  0.8062,\n",
      "          1.0788,  0.9907,  1.7185,  0.7997,  1.5773,  1.8011,  1.0211,  1.3464,\n",
      "          1.1691,  1.1230,  0.8060,  0.9062,  1.0608,  1.0642,  0.5162,  0.5950,\n",
      "          0.3727,  1.4240,  0.7236,  1.8883,  0.8696,  0.6125,  0.8155,  0.7598,\n",
      "          1.8282,  0.7009,  0.5921,  1.4706,  0.8728,  1.0686,  0.8170,  1.5191,\n",
      "          1.0469,  1.1284],\n",
      "        [ 0.3588,  0.4352, -2.0879,  0.0512,  0.4527,  0.5393, -2.6304,  0.5208,\n",
      "         -0.3722, -2.7530, -0.3099,  0.7718,  0.2092,  0.4319, -0.2189, -0.1109,\n",
      "         -1.7710, -0.0156, -1.5615,  0.0657,  0.1822, -0.2313,  0.1819,  0.5672,\n",
      "         -2.5489, -2.7455, -2.8526,  0.4133,  0.1816,  0.1072, -1.1330, -0.4517,\n",
      "         -0.7848, -2.9527, -1.9234,  0.8472,  0.1238, -0.3279, -0.0355, -0.0902,\n",
      "         -0.1162, -0.0683, -0.3192,  1.0339,  0.1465,  0.5195, -0.0288, -2.8443,\n",
      "          0.0642,  0.5394],\n",
      "        [ 0.3247,  0.5873, -2.1782,  0.0660,  0.5400,  0.5849, -2.9411,  0.3484,\n",
      "         -0.2962, -2.8148, -0.3841,  0.7472,  0.0954,  0.5182, -0.0199,  0.0221,\n",
      "         -2.0889,  0.1671, -1.7044,  0.0487,  0.3098,  0.0356,  0.1515,  0.5687,\n",
      "         -2.6919, -3.0164, -2.8723,  0.3267,  0.2430,  0.0906, -1.1189, -0.2514,\n",
      "         -0.8823, -3.0205, -2.2694,  0.7798,  0.0893, -0.5291,  0.1122, -0.1570,\n",
      "          0.0235, -0.3411, -0.2500,  0.9201,  0.2042,  0.3320, -0.0917, -2.9738,\n",
      "          0.0081,  0.5773],\n",
      "        [ 1.6172,  1.3803,  1.0122,  1.0700,  1.5042,  1.9373,  1.2449,  1.2681,\n",
      "          0.7160,  1.1830,  0.7483,  1.3933,  1.0326,  1.2740,  0.7462,  0.9488,\n",
      "          0.9872,  0.7832,  1.9370,  0.7421,  1.5386,  1.7365,  1.0714,  1.3092,\n",
      "          1.0371,  1.0463,  1.0234,  1.1173,  0.9532,  1.2540,  0.5440,  0.6923,\n",
      "          0.4924,  1.3818,  0.7149,  1.8566,  0.9286,  0.7944,  1.0316,  0.9458,\n",
      "          1.7638,  0.7591,  0.7682,  1.6797,  0.8961,  1.1980,  0.7921,  1.4609,\n",
      "          1.1865,  1.1697],\n",
      "        [ 0.4119,  0.5754, -1.8467,  0.2211,  0.3587,  0.3155, -2.5207,  0.3405,\n",
      "         -0.3298, -2.6204, -0.3204,  0.7300,  0.1047,  0.4059,  0.0216, -0.1189,\n",
      "         -1.8514,  0.0571, -1.6603,  0.0875,  0.1516, -0.1226,  0.3715,  0.4195,\n",
      "         -2.4502, -2.5872, -2.6478,  0.3499,  0.3908,  0.1041, -0.8540, -0.3911,\n",
      "         -0.8164, -2.9186, -2.0364,  0.7952,  0.1157, -0.2935, -0.0202, -0.0781,\n",
      "         -0.0494, -0.2903, -0.2513,  0.9914,  0.0809,  0.4827, -0.0855, -2.7726,\n",
      "          0.0130,  0.4718],\n",
      "        [ 0.4787,  0.5822, -2.1937,  0.1321,  0.5541,  0.6011, -2.6969,  0.4794,\n",
      "         -0.1242, -2.7567, -0.3360,  0.7018,  0.1307,  0.5779, -0.2689, -0.0313,\n",
      "         -1.9970,  0.0706, -1.4952,  0.0245,  0.1419, -0.0580,  0.2857,  0.4913,\n",
      "         -2.6387, -2.8234, -3.0200,  0.2274,  0.0840,  0.1373, -0.9476, -0.2535,\n",
      "         -0.8397, -3.0886, -2.1817,  0.8318, -0.0148, -0.5068, -0.0125, -0.0232,\n",
      "          0.0695, -0.0843, -0.2973,  0.8346,  0.0860,  0.2153, -0.1036, -3.0391,\n",
      "          0.2749,  0.5075],\n",
      "        [ 1.4624,  1.1458,  1.0642,  0.8846,  1.6506,  1.7984,  1.0808,  1.2465,\n",
      "          0.7535,  0.8892,  0.5576,  1.3444,  1.0106,  1.4006,  0.7320,  0.8664,\n",
      "          1.0612,  1.0070,  1.6383,  0.6938,  1.5688,  1.6688,  0.8635,  1.3700,\n",
      "          1.0696,  1.1853,  0.9767,  1.0958,  0.9543,  1.2597,  0.6683,  0.8019,\n",
      "          0.4014,  1.4005,  0.5433,  1.7008,  0.8607,  0.7045,  0.8101,  0.7525,\n",
      "          1.6664,  0.7788,  0.5659,  1.5267,  1.0148,  1.0771,  0.9425,  1.3655,\n",
      "          0.9957,  1.2925],\n",
      "        [ 1.3144,  1.1624,  0.9505,  0.9465,  1.7116,  1.7374,  1.0582,  1.1475,\n",
      "          0.7939,  0.9536,  0.5482,  1.1423,  0.9376,  1.4049,  0.6827,  0.9142,\n",
      "          0.9403,  0.9192,  1.6926,  0.8804,  1.5485,  1.7385,  1.0486,  1.3808,\n",
      "          1.0168,  0.9550,  1.0529,  0.9606,  0.9139,  1.1021,  0.6126,  0.7451,\n",
      "          0.3365,  1.3219,  0.6023,  1.7375,  0.9031,  0.6077,  0.8364,  0.8871,\n",
      "          1.6653,  0.8294,  0.7218,  1.6140,  0.9560,  0.9861,  0.9609,  1.5644,\n",
      "          0.8769,  1.1400]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1.3712, 0.6512, 1.0042, 0.5709, 1.2926, 0.6448, 0.5271, 0.2758, 1.1834,\n",
      "        1.1313, 1.2712, 1.1380, 1.0971, 0.3153, 0.7692, 1.3942, 0.5371, 0.0983,\n",
      "        1.0655, 1.1729], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.4017, -6.8699,  1.2680, -6.4331,  1.1642, -6.2867, -6.5038, -6.4106,\n",
      "          1.3250,  1.2499,  1.2125,  1.2299,  1.2459, -6.5626, -6.8067,  1.1372,\n",
      "         -6.5172, -6.6171,  1.3041,  1.3307]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1.0264], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
