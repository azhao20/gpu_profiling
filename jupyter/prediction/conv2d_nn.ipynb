{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
    "\n",
    "from utils.prediction_utils import *\n",
    "\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim import Adam\n",
    "\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/Desktop/Harvard/idreos-research/gpu_profiling/utils/time_utils.py:216: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dfs = pd.concat(dfs, axis=0, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 685125 entries, 0 to 685124\n",
      "Data columns (total 16 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   b             685125 non-null  int64  \n",
      " 1   in_channels   685125 non-null  int64  \n",
      " 2   iH            685125 non-null  int64  \n",
      " 3   iW            685125 non-null  int64  \n",
      " 4   out_channels  685125 non-null  int64  \n",
      " 5   groups        685125 non-null  int64  \n",
      " 6   kH            685125 non-null  int64  \n",
      " 7   kW            685125 non-null  int64  \n",
      " 8   stride        685125 non-null  int64  \n",
      " 9   dilation      685125 non-null  int64  \n",
      " 10  gflops        685125 non-null  float64\n",
      " 11  dtype_16      685125 non-null  bool   \n",
      " 12  dtype_32      685125 non-null  bool   \n",
      " 13  dtype_b16     685125 non-null  bool   \n",
      " 14  transposed_0  685125 non-null  bool   \n",
      " 15  transposed_1  685125 non-null  bool   \n",
      "dtypes: bool(5), float64(1), int64(10)\n",
      "memory usage: 60.8 MB\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"/Users/andrew/Desktop/Harvard/idreos-research/gpu_profiling\"\n",
    "X, y = get_data(\"conv2d\", base_dir, sample_rate=1.0)\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([X, y], axis=1)\n",
    "df = df.query(\"time > 0\").dropna()\n",
    "X, y = df.drop([\"time\"], axis=1), df[\"time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 444058 entries, 178626 to 121958\n",
      "Data columns (total 16 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   b             444058 non-null  int64  \n",
      " 1   in_channels   444058 non-null  int64  \n",
      " 2   iH            444058 non-null  int64  \n",
      " 3   iW            444058 non-null  int64  \n",
      " 4   out_channels  444058 non-null  int64  \n",
      " 5   groups        444058 non-null  int64  \n",
      " 6   kH            444058 non-null  int64  \n",
      " 7   kW            444058 non-null  int64  \n",
      " 8   stride        444058 non-null  int64  \n",
      " 9   dilation      444058 non-null  int64  \n",
      " 10  gflops        444058 non-null  float64\n",
      " 11  dtype_16      444058 non-null  bool   \n",
      " 12  dtype_32      444058 non-null  bool   \n",
      " 13  dtype_b16     444058 non-null  bool   \n",
      " 14  transposed_0  444058 non-null  bool   \n",
      " 15  transposed_1  444058 non-null  bool   \n",
      "dtypes: bool(5), float64(1), int64(10)\n",
      "memory usage: 42.8 MB\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = get_train_test_split(X, y, return_concat=False)\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "class PredictionNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size):\n",
    "        super(PredictionNetwork, self).__init__()\n",
    "        layers = []\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_size = hidden_size\n",
    "        layers.append(nn.Dropout(p=0.5))  # Dropout before the output layer\n",
    "        layers.append(nn.Linear(hidden_layers[-1], output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle_loss(y_pred, y_actual):\n",
    "    return torch.sqrt(torch.mean(torch.square(torch.log1p(y_pred) - torch.log1p(y_actual))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=300):\n",
    "    criterion = rmsle_loss\n",
    "    optimizer = Adam(model.parameters(), lr=0.1, weight_decay=1e-5)  # L2 regularization\n",
    "    scheduler = StepLR(optimizer, step_size=40, gamma=0.5)  # Learning rate decay\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for data, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "        # Optionally validate the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            validation_loss = 0\n",
    "            for data, targets in val_loader:\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "                validation_loss += loss.item()\n",
    "            print(f'Validation Loss: {validation_loss / len(val_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your data loaders\n",
    "# train_loader, val_loader = setup_data_loaders()\n",
    "\n",
    "# Assuming a certain input size and output size\n",
    "model = PredictionNetwork(input_size=10, hidden_layers=[64, 128], output_size=1)\n",
    "# train_model(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95155,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train_scaled.shape\n",
    "# X_val_scaled.shape\n",
    "# X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    y_train_t = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "    X_val_t = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "    y_val_t = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating datasets\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "val_dataset = TensorDataset(X_val_t, y_val_t)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network Architecture\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train_t.shape[1], 20)\n",
    "        self.fc2 = nn.Linear(50, 20)\n",
    "        self.fc3 = nn.Linear(20, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class EnhancedRegressionNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EnhancedRegressionNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train_t.shape[1], 100)  # Increase the number of neurons\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.fc3 = nn.Linear(50, 20)\n",
    "        self.fc4 = nn.Linear(20, 10)\n",
    "        self.fc5 = nn.Linear(10, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.3)  # Adding dropout for regularization\n",
    "\n",
    "        # Optional: Include batch normalization layers\n",
    "        self.batch_norm1 = nn.BatchNorm1d(100)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(50)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(20)\n",
    "        self.batch_norm4 = nn.BatchNorm1d(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.batch_norm1(self.fc1(x)))\n",
    "        x = self.dropout(x)  # Applying dropout\n",
    "        x = self.relu(self.batch_norm2(self.fc2(x)))\n",
    "        x = self.relu(self.batch_norm3(self.fc3(x)))\n",
    "        x = self.relu(self.batch_norm4(self.fc4(x)))\n",
    "        x = self.fc5(x)  # Output layer without\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 634369 entries, 0 to 684936\n",
      "Data columns (total 16 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   b             634369 non-null  int64  \n",
      " 1   in_channels   634369 non-null  int64  \n",
      " 2   iH            634369 non-null  int64  \n",
      " 3   iW            634369 non-null  int64  \n",
      " 4   out_channels  634369 non-null  int64  \n",
      " 5   groups        634369 non-null  int64  \n",
      " 6   kH            634369 non-null  int64  \n",
      " 7   kW            634369 non-null  int64  \n",
      " 8   stride        634369 non-null  int64  \n",
      " 9   dilation      634369 non-null  int64  \n",
      " 10  gflops        634369 non-null  float64\n",
      " 11  dtype_16      634369 non-null  bool   \n",
      " 12  dtype_32      634369 non-null  bool   \n",
      " 13  dtype_b16     634369 non-null  bool   \n",
      " 14  transposed_0  634369 non-null  bool   \n",
      " 15  transposed_1  634369 non-null  bool   \n",
      "dtypes: bool(5), float64(1), int64(10)\n",
      "memory usage: 61.1 MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 694344.6250, Validation Loss: 500541.3714\n",
      "Epoch 2, Train Loss: 350.9424, Validation Loss: 493101.3078\n",
      "Epoch 3, Train Loss: 95348.5391, Validation Loss: 482951.1448\n",
      "Epoch 4, Train Loss: 155022.1875, Validation Loss: 474194.9193\n",
      "Epoch 5, Train Loss: 734.2679, Validation Loss: 457000.2978\n",
      "Epoch 6, Train Loss: 17726.1074, Validation Loss: 418684.1120\n",
      "Epoch 7, Train Loss: 1518.5846, Validation Loss: 397694.7514\n",
      "Epoch 8, Train Loss: 7547.5752, Validation Loss: 390082.7111\n",
      "Epoch 9, Train Loss: 4378.8276, Validation Loss: 360240.3024\n",
      "Epoch 10, Train Loss: 6034.9473, Validation Loss: 315839.8117\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     25\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 26\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Validation loop\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/research/lib/python3.10/site-packages/torch/optim/optimizer.py:479\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    475\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    476\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m             )\n\u001b[0;32m--> 479\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    482\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/research/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/research/lib/python3.10/site-packages/torch/optim/adam.py:231\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    219\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    221\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    222\u001b[0m         group,\n\u001b[1;32m    223\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m         state_steps,\n\u001b[1;32m    229\u001b[0m     )\n\u001b[0;32m--> 231\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/miniconda3/envs/research/lib/python3.10/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/research/lib/python3.10/site-packages/torch/optim/adam.py:776\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    774\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 776\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/research/lib/python3.10/site-packages/torch/optim/adam.py:438\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    436\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 438\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    440\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO: write a script that\n",
    "1) Takes a set of learning rates\n",
    "2) Takes a potential model name\n",
    "3) outputs a bunch of validation losses in a new folder.\n",
    "\"\"\"\n",
    "lr = 1e-4\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = EnhancedRegressionNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "validation_losses = []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss = loss.item()\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, targets.view(-1, 1)).item()\n",
    "        val_loss /= len(val_loader)\n",
    "    validation_losses.append(val_loss)\n",
    "    # if (epoch + 1) % 2 == 0:\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 7.3877e-01,  3.1969e-01,  5.8624e-01,  6.3714e-01,  3.1101e-02,\n",
      "         -3.1253e+00,  2.5411e-01,  4.2812e-01,  2.1393e-37,  2.2994e-37,\n",
      "          7.5621e-03, -5.7961e-01,  1.4561e+00, -1.4630e+00,  1.1861e-01,\n",
      "         -1.2178e-01],\n",
      "        [ 9.1689e-01,  9.7033e-01,  7.0651e-01,  1.0657e+00,  1.9568e-01,\n",
      "         -1.5035e+00,  5.1491e-01,  4.2059e-01,  1.7083e-37,  2.7001e-37,\n",
      "          1.4968e-02, -2.9886e-01,  1.8517e+00, -1.8406e+00, -5.3701e-01,\n",
      "          8.5764e-01],\n",
      "        [ 3.8090e-03,  5.0351e-03,  2.1848e-03,  2.4860e-03,  4.2933e-03,\n",
      "         -2.7401e+00,  2.3639e-03, -8.9123e-04,  2.0427e-37, -4.2463e-38,\n",
      "          4.2975e-01,  1.4509e-01,  1.4507e-01,  1.4317e-01,  3.5634e-02,\n",
      "          3.7120e-02],\n",
      "        [ 6.2758e-01,  6.8996e-01,  8.1576e-01,  7.0331e-01,  8.7013e-02,\n",
      "         -8.9521e-01,  2.6054e-01,  2.2520e-01,  5.6746e-37, -2.0159e-37,\n",
      "          4.3432e-01, -2.3801e-01,  9.1401e-01, -1.2021e+00, -7.4300e-01,\n",
      "          4.3373e-01],\n",
      "        [ 5.1709e-01,  2.3051e-01,  3.1979e-01,  4.8097e-01,  1.0441e-01,\n",
      "         -3.7867e+00,  3.5079e-01,  6.4978e-01,  2.2564e-37, -8.3048e-38,\n",
      "          4.5706e-03, -8.0745e-01,  1.7686e+00, -1.4246e+00, -6.8049e-02,\n",
      "         -3.7269e-01],\n",
      "        [ 4.9144e-01,  7.8607e-01,  4.5194e-01,  4.8205e-01, -1.0544e-01,\n",
      "         -4.1970e+00,  5.5887e-01,  4.4517e-01, -4.3498e-37,  2.3073e-37,\n",
      "         -6.5340e-02, -1.2725e+00,  1.9956e+00, -1.1539e+00,  4.0537e-01,\n",
      "         -4.6714e-01],\n",
      "        [ 7.8299e-03,  2.9731e-02,  1.4406e-02,  1.2801e-02,  3.6541e-02,\n",
      "         -3.3198e+00,  8.9921e-03,  3.0470e-03, -2.1579e-37,  2.2775e-37,\n",
      "          4.9008e-01,  4.4065e-02,  8.7554e-02,  4.1357e-02, -1.8965e-01,\n",
      "         -1.8597e-01],\n",
      "        [ 7.5459e-01,  5.7367e-01,  6.9390e-01,  1.0904e+00,  3.1082e-01,\n",
      "         -1.2388e+00,  9.2192e-01,  4.3690e-01, -4.6063e-37,  1.1617e-37,\n",
      "          1.7080e-01, -2.8642e-01,  1.6382e+00, -1.0998e+00, -7.9537e-01,\n",
      "          3.5216e-01],\n",
      "        [ 4.2761e-01,  5.1359e-01,  5.7677e-01,  6.8281e-01, -7.2674e-02,\n",
      "         -3.4870e-01,  2.6098e-01,  2.1522e-01,  2.0902e-37,  3.3918e-37,\n",
      "          8.4762e-01, -8.0894e-01,  4.3734e-01, -8.2825e-02, -4.2433e-01,\n",
      "          5.6640e-01],\n",
      "        [ 8.9166e-03,  4.2526e-02,  1.7016e-02,  1.2412e-02,  6.8253e-02,\n",
      "         -2.7165e+00,  1.5053e-02,  5.5756e-03, -1.5904e-37, -4.2018e-37,\n",
      "          4.0197e-01, -1.8182e-01,  1.4362e-02, -1.0155e-01, -1.3305e-01,\n",
      "         -1.8077e-01],\n",
      "        [ 6.5836e-01,  4.0126e-02,  6.9941e-02,  1.0980e-01,  4.2820e-02,\n",
      "         -7.6473e-01, -7.2722e-02,  1.6648e-01,  1.2448e-37, -2.2914e-37,\n",
      "          7.0005e-01, -1.1134e-01,  4.9070e-01, -6.4026e-01, -6.9023e-01,\n",
      "          3.0797e-01],\n",
      "        [ 1.0252e+00,  9.2707e-01,  9.4823e-01,  7.3006e-01,  5.1374e-01,\n",
      "         -1.7904e+00,  5.8520e-01,  3.2102e-01, -2.2935e-37, -2.4086e-37,\n",
      "         -1.0217e-01, -5.5806e-01,  1.3579e+00, -1.1799e+00, -7.2658e-01,\n",
      "          9.1930e-01],\n",
      "        [ 8.4399e-01,  2.1868e-01,  2.7131e-01,  7.7202e-01,  1.2193e-01,\n",
      "         -1.0730e+00,  4.4170e-01,  5.7172e-01,  1.9515e-37, -3.9268e-37,\n",
      "          4.2564e-01,  9.9961e-02,  1.4951e+00, -1.7117e+00, -5.7062e-01,\n",
      "          4.1307e-01],\n",
      "        [ 8.8371e-01,  6.0344e-01,  4.0559e-01,  8.9800e-01, -2.9213e-01,\n",
      "         -1.9122e+00,  6.9197e-01,  2.5283e-01, -1.9018e-37, -8.7931e-38,\n",
      "          1.1053e-01, -4.9006e-01,  1.5053e+00, -1.5436e+00, -3.9466e-01,\n",
      "          7.9852e-01],\n",
      "        [ 7.5838e-01,  2.8403e-01,  4.0522e-01,  6.0064e-01, -3.8730e-02,\n",
      "         -1.0997e+00,  7.0593e-01,  5.5704e-01, -1.8573e-37,  2.0256e-37,\n",
      "          7.0293e-01,  5.9709e-02,  3.7586e-01, -8.4603e-01, -2.7730e-01,\n",
      "          3.7000e-01],\n",
      "        [ 5.0289e-01,  3.7407e-01,  2.9555e-01,  4.1422e-01,  1.3277e-01,\n",
      "         -1.0098e+00,  4.2870e-01,  2.8082e-01,  2.0939e-37, -2.1901e-37,\n",
      "          4.0947e-01,  7.8106e-02,  1.0892e+00, -1.8205e+00, -4.2782e-01,\n",
      "          1.0470e-01],\n",
      "        [ 6.6947e-03,  2.5542e-02,  1.0469e-02,  8.0997e-03,  3.1959e-02,\n",
      "         -2.7499e+00,  9.1177e-03,  2.0367e-03,  2.1253e-37, -2.0845e-38,\n",
      "          4.1867e-01, -9.0092e-02,  6.6618e-02,  2.0190e-02, -1.5278e-01,\n",
      "         -1.6847e-01],\n",
      "        [ 8.3835e-01,  6.6933e-01,  9.3156e-01,  1.1210e+00,  2.4551e-01,\n",
      "         -4.5031e-01,  1.7424e-01,  1.7650e-01,  2.3666e-37,  2.9044e-37,\n",
      "          4.1955e-01, -2.0576e-01,  8.2421e-01, -8.6273e-01, -3.5688e-01,\n",
      "          3.1571e-01],\n",
      "        [ 5.8199e-02,  3.2816e-01,  1.6469e-01,  1.6811e-01,  1.5052e-01,\n",
      "         -3.1545e+00,  1.1578e-01,  9.2304e-02, -2.0072e-37,  4.1214e-38,\n",
      "          1.9779e-01,  1.3349e-01,  2.7243e-01,  1.9696e-01,  1.3073e-01,\n",
      "          5.0948e-03],\n",
      "        [ 5.0720e-01,  8.3036e-01,  7.1454e-01,  9.0781e-01, -7.5129e-02,\n",
      "         -3.8618e-01,  5.3241e-01,  4.2656e-01, -2.4192e-37,  1.5976e-37,\n",
      "          5.8713e-01, -1.9197e-01,  6.8722e-01, -1.0643e+00, -1.4151e-01,\n",
      "          1.7772e-01],\n",
      "        [ 4.6161e-01,  2.6707e-01,  6.3590e-02,  1.1164e-01,  9.6465e-02,\n",
      "         -3.4354e+00,  1.6452e-01,  3.2794e-01, -2.3577e-37, -2.3941e-37,\n",
      "          5.7067e-02, -7.6436e-01,  1.2863e+00, -7.8560e-01,  2.7169e-01,\n",
      "         -2.5498e-01],\n",
      "        [ 1.5124e-01,  8.3422e-02, -1.8801e-01, -1.3720e-01,  6.6119e-01,\n",
      "         -4.2010e+00,  5.4603e-01,  3.2411e-01, -1.3331e-37, -3.0303e-37,\n",
      "          8.6830e-02, -8.7039e-01,  1.1955e+00, -9.1607e-01,  2.8106e-01,\n",
      "         -5.4656e-01],\n",
      "        [ 5.2953e-01,  6.1870e-01,  5.9012e-01,  8.3953e-01,  2.2161e-01,\n",
      "         -1.0703e+00,  6.7953e-01,  3.5109e-01,  2.2209e-37, -4.1520e-37,\n",
      "          3.8961e-01, -8.1840e-01,  1.0409e+00, -5.4436e-01, -5.8449e-01,\n",
      "          6.3919e-01],\n",
      "        [ 9.1258e-01,  4.5215e-01,  5.9440e-01,  8.1463e-01,  8.4563e-02,\n",
      "         -2.5472e+00,  4.5006e-01,  6.5627e-01, -2.1532e-37, -2.1027e-37,\n",
      "         -5.2997e-02, -1.0337e+00,  2.0632e+00, -1.2549e+00,  9.4586e-02,\n",
      "          5.8671e-02],\n",
      "        [ 8.5662e-03,  5.7749e-02,  1.9095e-02,  1.1841e-02,  9.1940e-02,\n",
      "         -3.0619e+00,  1.7703e-02,  8.6362e-03, -8.2625e-38, -2.1599e-37,\n",
      "          4.7300e-01, -1.6325e-01,  1.3046e-01, -4.0005e-02,  2.7035e-01,\n",
      "          1.7046e-01],\n",
      "        [ 5.2319e-03,  3.4586e-02,  1.1793e-02,  8.5899e-03,  5.4614e-02,\n",
      "         -3.0749e+00,  1.0475e-02,  2.2217e-03, -2.1267e-37, -2.1707e-37,\n",
      "          4.5842e-01,  1.2130e-01,  2.3614e-01,  1.3825e-01,  2.6277e-02,\n",
      "         -1.5270e-03],\n",
      "        [ 1.3925e-03,  1.1996e-03, -3.1393e-05,  5.8541e-04,  1.5926e-04,\n",
      "         -2.8975e+00,  4.3569e-04, -1.1632e-03,  2.3174e-37,  2.2055e-37,\n",
      "          4.7309e-01,  5.0010e-02,  4.9218e-02,  4.8121e-02,  4.9871e-02,\n",
      "          4.7808e-02],\n",
      "        [ 5.4957e-01,  8.5861e-01,  6.0664e-01,  8.5166e-01,  2.3224e-01,\n",
      "         -8.7898e-01,  4.0039e-01,  2.7989e-01,  2.3025e-37, -2.4441e-37,\n",
      "          3.4306e-01, -5.0722e-01,  1.5585e+00, -9.3288e-01, -6.7829e-01,\n",
      "          7.5000e-01],\n",
      "        [ 7.2812e-01,  5.3403e-01,  5.1131e-01,  7.3751e-01,  5.3444e-02,\n",
      "         -1.3450e+00,  8.3643e-01,  7.2233e-01,  2.1182e-37,  2.1868e-37,\n",
      "          3.2150e-01, -8.4665e-01,  1.5269e+00, -9.9037e-01,  4.5697e-03,\n",
      "          1.4537e-02],\n",
      "        [ 4.1770e-01,  2.5522e-01, -1.2527e-01,  2.4623e-02,  1.0490e-01,\n",
      "         -2.6714e+00,  3.4729e-01,  3.5644e-01, -2.2417e-37,  1.2959e-37,\n",
      "          1.3208e-01, -4.7529e-01,  1.6139e+00, -9.0137e-01,  2.0722e-02,\n",
      "         -2.8777e-01],\n",
      "        [ 1.0249e-02,  4.1963e-02,  4.7361e-02,  3.6020e-02,  1.3022e-02,\n",
      "         -1.6624e+00,  7.8988e-03, -1.0259e-02, -2.0921e-37,  2.3837e-37,\n",
      "          5.4823e-01, -3.5415e-01,  1.6240e-01,  5.8250e-02, -2.4012e-02,\n",
      "         -1.8209e-01],\n",
      "        [ 2.9851e-01,  2.1590e-01,  5.3730e-01,  5.9138e-01, -3.5673e-01,\n",
      "         -1.4403e-01,  8.2786e-02,  3.9066e-03, -2.0627e-37,  2.3783e-37,\n",
      "          6.5442e-01, -4.5888e-01,  4.9273e-01, -4.9127e-01, -1.5788e-01,\n",
      "          1.6401e-01],\n",
      "        [ 1.9364e-01,  1.9644e-01,  3.4874e-01,  3.8028e-01, -2.5359e-01,\n",
      "         -5.5185e-01, -2.7151e-02, -6.4768e-02,  8.8021e-38, -2.2855e-37,\n",
      "          9.9928e-01,  1.9481e-01,  1.6011e-01, -4.0145e-01, -1.2273e-01,\n",
      "         -4.1418e-02],\n",
      "        [ 1.6108e-02,  6.4417e-02,  2.9553e-02,  2.7143e-02,  7.8468e-02,\n",
      "         -3.2148e+00,  1.7240e-02,  8.0184e-03, -2.3638e-37,  1.6075e-37,\n",
      "          4.1086e-01, -1.4664e-01, -6.2280e-02, -1.5211e-01, -6.2781e-02,\n",
      "         -5.9523e-02],\n",
      "        [ 1.0755e-03, -9.9012e-04, -2.2265e-03, -1.6692e-03, -1.7765e-03,\n",
      "         -2.2459e+00,  1.7807e-03, -3.4282e-04, -1.8677e-37, -2.1337e-37,\n",
      "          5.9552e-01, -6.3671e-02,  2.8149e-01,  2.7932e-03, -2.4492e-01,\n",
      "         -2.5230e-01],\n",
      "        [ 6.9922e-01,  5.6026e-01,  7.5511e-01,  9.1984e-01,  3.0233e-01,\n",
      "         -3.9113e+00,  7.6047e-01,  5.0461e-01, -1.9627e-37, -2.4741e-37,\n",
      "         -1.8041e-01, -6.6551e-01,  1.6634e+00, -9.9128e-01,  4.3899e-01,\n",
      "         -1.0219e-01],\n",
      "        [ 6.2233e-01,  4.3742e-01,  2.9808e-01,  5.3491e-01, -2.2678e-01,\n",
      "         -8.4539e-01,  8.1973e-01,  5.1260e-01, -9.0144e-38,  2.3149e-37,\n",
      "          4.7009e-01, -5.7567e-02,  1.0976e+00, -1.3786e+00, -8.7087e-01,\n",
      "          5.5785e-01],\n",
      "        [ 5.3193e-01,  2.1530e-01,  6.7171e-01,  8.5911e-01,  4.6380e-02,\n",
      "         -2.9638e-01,  2.4207e-01,  2.0374e-01, -3.2922e-38,  2.0892e-37,\n",
      "          9.2675e-01, -1.1466e-03, -1.2455e-02, -5.9608e-02,  5.3658e-02,\n",
      "         -6.6695e-02],\n",
      "        [ 3.5110e-01,  4.1099e-01,  4.1445e-01,  5.7872e-01, -8.7097e-02,\n",
      "         -1.3121e+00,  7.1215e-01,  3.2707e-02,  2.2138e-37, -2.2670e-37,\n",
      "          3.8200e-01, -2.7535e-01,  9.6586e-01, -9.1894e-01, -6.2050e-02,\n",
      "          1.5687e-01],\n",
      "        [ 4.3192e-01,  6.0332e-01,  6.7297e-01,  9.2857e-01, -2.7438e-01,\n",
      "         -6.5744e-01,  3.7302e-01,  1.8403e-01,  3.2704e-37, -2.7626e-37,\n",
      "          5.3581e-01, -3.2379e-02,  4.2973e-01, -5.8737e-01, -4.7661e-01,\n",
      "          2.8849e-01],\n",
      "        [-2.1813e-01,  8.3213e-01,  5.3600e-01,  5.2963e-01,  2.0221e-01,\n",
      "         -3.9062e+00,  3.1965e-01,  4.0960e-01,  2.0772e-37, -5.8030e-38,\n",
      "          5.2627e-02, -1.0721e+00,  1.5221e+00, -7.0572e-01,  4.7287e-01,\n",
      "         -7.2600e-01],\n",
      "        [ 7.3804e-01,  4.8238e-01,  6.0269e-01,  5.0655e-01,  1.4478e-02,\n",
      "         -5.4613e-01,  3.1863e-01,  2.7878e-02,  2.0362e-37,  1.0222e-37,\n",
      "          7.9994e-01, -3.5585e-01,  6.2344e-01, -2.7705e-01, -4.1426e-01,\n",
      "          2.4486e-01],\n",
      "        [ 2.3233e-01,  4.8498e-01,  3.5524e-01,  4.9012e-01,  6.3676e-02,\n",
      "         -5.9332e-01,  3.6435e-01,  2.8085e-02, -2.0984e-37,  1.4430e-37,\n",
      "          8.6684e-01, -8.1330e-02,  3.8079e-01, -7.2736e-01, -3.0351e-01,\n",
      "          4.5931e-01],\n",
      "        [ 7.7034e-01,  1.0600e+00,  9.2763e-01,  1.2964e+00,  2.3519e-01,\n",
      "         -2.4314e+00,  8.9009e-01,  6.1663e-01,  2.3072e-37, -2.1073e-37,\n",
      "         -2.8003e-01, -3.9992e-01,  8.7990e-01, -9.1802e-01, -1.5588e-01,\n",
      "          2.0654e-01],\n",
      "        [ 5.5061e-01,  4.4694e-01,  9.7714e-01,  7.8616e-01,  1.0598e-01,\n",
      "         -6.7348e-01,  5.2458e-01,  5.3815e-01,  2.3923e-37,  1.5110e-37,\n",
      "          3.6731e-01, -5.1210e-01,  1.0384e+00, -1.0546e+00, -3.6370e-01,\n",
      "          2.8274e-01],\n",
      "        [ 8.9910e-01,  5.6761e-01,  4.6522e-01,  8.8294e-01, -1.4076e-01,\n",
      "         -1.3295e+00,  8.5807e-01,  4.0992e-01, -1.8409e-37,  1.0120e-37,\n",
      "          1.8683e-01, -3.2546e-02,  1.5188e+00, -1.5451e+00, -7.6548e-01,\n",
      "          5.1203e-01],\n",
      "        [ 4.8711e-01,  5.6537e-01,  5.4653e-01,  5.4227e-01, -1.5495e-01,\n",
      "         -5.3549e-01,  1.7211e-01,  2.8605e-01, -1.8109e-37, -1.6460e-37,\n",
      "          4.4752e-01,  1.7437e-01,  9.2800e-01, -1.5023e+00, -1.6427e-01,\n",
      "          1.0561e-01],\n",
      "        [ 3.8626e-02,  8.9913e-02,  5.9994e-02,  5.7781e-02,  1.2673e-01,\n",
      "         -3.3425e+00,  2.7282e-02,  1.5514e-02, -1.3856e-37, -1.8948e-37,\n",
      "          3.6126e-01, -8.4588e-02, -1.5911e-02, -9.9308e-02,  3.2796e-02,\n",
      "          5.4800e-02],\n",
      "        [ 9.1154e-01,  5.6082e-01,  7.1291e-01,  1.0374e+00, -1.3986e-01,\n",
      "         -1.3246e+00,  5.0485e-01,  2.8036e-01,  2.0166e-37,  2.0584e-37,\n",
      "          4.1238e-01, -5.3816e-01,  8.0395e-01, -6.8109e-01, -5.4528e-01,\n",
      "          4.8285e-01],\n",
      "        [ 7.8546e-01,  1.0316e+00,  1.1051e+00,  1.1857e+00,  1.2375e-01,\n",
      "         -1.0774e+00,  4.7493e-01,  4.1569e-01, -3.2807e-37,  2.2485e-37,\n",
      "          1.3171e-01, -5.9929e-01,  1.2528e+00, -9.4606e-01, -5.3005e-01,\n",
      "          8.2143e-01]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 6.7919e-01,  1.1599e+00, -1.6994e+00,  1.1076e+00,  8.5101e-01,\n",
      "         8.5223e-01, -2.1012e+00,  1.0440e+00,  2.5644e-01, -1.8803e+00,\n",
      "        -5.7315e-01,  1.2784e+00,  6.5163e-01,  1.1472e+00,  5.8901e-01,\n",
      "        -1.1907e-01, -1.8339e+00,  1.0959e+00, -2.0413e+00,  1.3202e+00,\n",
      "        -3.3921e-02, -4.2208e-01,  1.4460e+00,  9.4281e-01, -2.1976e+00,\n",
      "        -2.0284e+00, -1.7942e+00,  1.4546e+00,  1.0891e+00, -2.0054e-03,\n",
      "        -1.4845e+00, -7.4392e-02, -6.3643e-01, -2.0752e+00, -1.7842e+00,\n",
      "         1.2098e+00,  8.0109e-01,  5.8165e-01,  4.9574e-01,  6.2867e-01,\n",
      "        -2.9204e-01,  7.1955e-01,  5.0701e-01,  1.1886e+00,  1.0596e+00,\n",
      "         1.3519e+00,  3.2659e-01, -2.1369e+00,  8.5520e-01,  1.6147e+00],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.3997,  1.2009,  0.9888,  0.8578,  1.7028,  1.8513,  1.2166,  1.0569,\n",
      "          0.5722,  0.9357,  0.7775,  1.2856,  1.0511,  1.4046,  0.6060,  1.0123,\n",
      "          0.8551,  0.8826,  1.7906,  0.8311,  1.3465,  1.7941,  0.9014,  1.3347,\n",
      "          1.1115,  1.0314,  0.9235,  0.9322,  0.8680,  1.1638,  0.6043,  0.5832,\n",
      "          0.3916,  1.2527,  0.5579,  1.8686,  1.0060,  0.6329,  0.8042,  0.7102,\n",
      "          1.7692,  0.7558,  0.6718,  1.4960,  0.9716,  1.1386,  0.7617,  1.3704,\n",
      "          1.0943,  1.1426],\n",
      "        [ 0.4693,  0.6097, -2.1236,  0.1043,  0.4507,  0.5309, -2.7987,  0.3240,\n",
      "         -0.1615, -2.8500, -0.4850,  0.7981,  0.0036,  0.5349, -0.1166, -0.1070,\n",
      "         -1.8507,  0.0099, -1.8008,  0.0408,  0.2263, -0.0845,  0.2574,  0.6265,\n",
      "         -2.6408, -2.8355, -2.8561,  0.2923,  0.1385,  0.2107, -1.2078, -0.2490,\n",
      "         -0.8302, -3.1786, -2.2126,  0.8981,  0.1775, -0.5299,  0.1672, -0.0227,\n",
      "         -0.0382, -0.2066, -0.3129,  1.0102,  0.2453,  0.3300, -0.0873, -2.9727,\n",
      "         -0.0109,  0.4098],\n",
      "        [ 1.4716,  1.2911,  1.0906,  0.9354,  1.6703,  1.8047,  1.1409,  1.0917,\n",
      "          0.5692,  1.1289,  0.5969,  1.3187,  0.8157,  1.2853,  0.7806,  0.9661,\n",
      "          0.9971,  0.9247,  1.7245,  0.6911,  1.3660,  1.7445,  1.0838,  1.4035,\n",
      "          0.9931,  0.9624,  0.8027,  0.9163,  0.8961,  1.1443,  0.6288,  0.5940,\n",
      "          0.5344,  1.2022,  0.7457,  1.7386,  0.8454,  0.6173,  0.8170,  0.9066,\n",
      "          1.7721,  0.6738,  0.7545,  1.5846,  1.0007,  1.1144,  0.9063,  1.4830,\n",
      "          1.0974,  1.2772],\n",
      "        [ 0.5089,  0.6675, -2.0329,  0.1503,  0.3537,  0.5174, -2.6406,  0.4378,\n",
      "         -0.2681, -2.9251, -0.1914,  0.5446,  0.2506,  0.5780, -0.0998,  0.1220,\n",
      "         -2.0086,  0.2190, -1.4894, -0.0521,  0.1717,  0.1383,  0.1396,  0.3935,\n",
      "         -2.6245, -2.9335, -3.0001,  0.1495,  0.2829,  0.2291, -1.1125, -0.3795,\n",
      "         -0.7892, -2.9647, -2.0630,  0.7028, -0.0860, -0.3508,  0.1016, -0.0904,\n",
      "          0.0347, -0.1324, -0.4038,  0.8478,  0.0567,  0.2988, -0.1151, -2.8397,\n",
      "          0.2330,  0.4636],\n",
      "        [ 1.3686,  1.3931,  1.1357,  0.9676,  1.5747,  1.8759,  1.0722,  1.0502,\n",
      "          0.7866,  0.9526,  0.7907,  1.4458,  0.9382,  1.2621,  0.6187,  0.9779,\n",
      "          0.8550,  0.8324,  1.9292,  0.8512,  1.5646,  1.6489,  1.1134,  1.3351,\n",
      "          1.0018,  1.0319,  0.8752,  1.0529,  1.0942,  1.1133,  0.7390,  0.7982,\n",
      "          0.5787,  1.4019,  0.7721,  1.8674,  0.9794,  0.7276,  0.7883,  0.7076,\n",
      "          1.8208,  0.7513,  0.7628,  1.4816,  0.9152,  1.2865,  1.0411,  1.4767,\n",
      "          0.9250,  1.2747],\n",
      "        [ 0.4219,  0.6385, -2.1656,  0.1230,  0.4489,  0.4858, -2.5137,  0.3059,\n",
      "         -0.3262, -2.6900, -0.4608,  0.6106,  0.1862,  0.5865, -0.1359, -0.1462,\n",
      "         -1.6940,  0.1811, -1.6837, -0.0862,  0.1064, -0.1128,  0.1604,  0.5377,\n",
      "         -2.4183, -2.8043, -2.7836,  0.1779,  0.3903,  0.1297, -0.9959, -0.4375,\n",
      "         -0.7662, -2.8978, -1.9778,  0.7430,  0.1039, -0.3184,  0.0430,  0.0154,\n",
      "         -0.0317, -0.1764, -0.2257,  0.9240,  0.1735,  0.3312,  0.0560, -2.9764,\n",
      "          0.0450,  0.5225],\n",
      "        [ 0.4589,  0.4782, -2.0765,  0.2278,  0.4390,  0.5135, -2.6482,  0.4701,\n",
      "         -0.3126, -2.7716, -0.1990,  0.7374,  0.0990,  0.4963, -0.0543, -0.0617,\n",
      "         -1.8547,  0.0080, -1.5614,  0.0944,  0.1496, -0.1919,  0.1798,  0.6084,\n",
      "         -2.3955, -2.8732, -2.7812,  0.1885,  0.1749,  0.1807, -1.1747, -0.3216,\n",
      "         -0.8110, -3.0593, -2.0021,  0.7064,  0.0485, -0.4038,  0.1379, -0.0565,\n",
      "         -0.1207, -0.2093, -0.3525,  0.9975,  0.2647,  0.4510, -0.1730, -2.8197,\n",
      "          0.2295,  0.4543],\n",
      "        [ 0.4791,  0.6217, -2.1225,  0.0318,  0.4372,  0.3981, -2.5494,  0.3382,\n",
      "         -0.2525, -2.6494, -0.2637,  0.7021, -0.0179,  0.4616, -0.1853, -0.0542,\n",
      "         -1.7368,  0.0684, -1.5587, -0.1088,  0.2535, -0.3082,  0.2178,  0.5455,\n",
      "         -2.4046, -2.7535, -2.7944,  0.3207,  0.2790,  0.0698, -1.0684, -0.2409,\n",
      "         -0.7893, -2.8732, -1.9760,  0.7967,  0.1304, -0.1950,  0.1226, -0.1068,\n",
      "         -0.0202, -0.1953, -0.1345,  1.0074,  0.2403,  0.3398, -0.0926, -3.0234,\n",
      "          0.2271,  0.4643],\n",
      "        [ 1.3668,  1.0720,  0.8312,  0.9919,  1.4925,  1.8221,  0.9963,  1.1562,\n",
      "          0.5808,  0.9676,  0.7782,  1.3591,  0.9130,  1.3061,  0.8141,  0.7896,\n",
      "          0.8205,  0.8559,  1.6742,  0.7648,  1.5763,  1.6926,  1.0285,  1.3648,\n",
      "          1.0218,  1.0771,  0.9120,  0.8839,  0.9852,  1.1560,  0.6271,  0.5546,\n",
      "          0.5993,  1.3843,  0.7157,  1.8453,  0.9659,  0.7139,  0.9231,  0.8330,\n",
      "          1.7338,  0.7389,  0.5114,  1.6117,  1.0660,  1.1636,  1.0100,  1.4754,\n",
      "          1.1268,  1.2123],\n",
      "        [ 1.3667,  1.3490,  0.9263,  0.9666,  1.5588,  2.0081,  1.0768,  1.1455,\n",
      "          0.8349,  0.9244,  0.6162,  1.2466,  1.0626,  1.2716,  0.8519,  0.8359,\n",
      "          0.8479,  1.0508,  1.7604,  0.8145,  1.3913,  1.8454,  0.9012,  1.3820,\n",
      "          1.1263,  1.1029,  0.9980,  0.9445,  1.0665,  1.1648,  0.6801,  0.6033,\n",
      "          0.3648,  1.2178,  0.7067,  1.7583,  0.8853,  0.5810,  0.9018,  0.8122,\n",
      "          1.8359,  0.6450,  0.6343,  1.5326,  1.1407,  1.2393,  0.9875,  1.6258,\n",
      "          1.0140,  1.1200],\n",
      "        [ 1.3808,  1.3452,  0.9518,  0.9107,  1.5984,  1.8329,  1.0511,  1.1632,\n",
      "          0.6210,  1.0416,  0.7798,  1.4340,  1.0095,  1.4886,  0.8348,  0.8923,\n",
      "          0.9439,  0.7891,  1.7952,  0.7611,  1.5230,  1.7168,  1.0158,  1.3537,\n",
      "          1.1058,  1.0519,  0.9013,  1.0849,  1.1439,  1.3156,  0.5152,  0.7885,\n",
      "          0.4418,  1.2697,  0.6004,  1.7898,  1.0375,  0.6554,  0.8839,  0.9019,\n",
      "          1.9838,  0.6829,  0.7091,  1.6078,  0.9408,  1.0256,  0.8039,  1.6013,\n",
      "          1.0809,  1.2236],\n",
      "        [ 1.3389,  1.3492,  1.0274,  0.7992,  1.4826,  1.9423,  1.0667,  1.0143,\n",
      "          0.8055,  0.9247,  0.6016,  1.2253,  0.9316,  1.2278,  0.6283,  0.9212,\n",
      "          0.8969,  0.9841,  1.7023,  0.9363,  1.4768,  1.6089,  1.0864,  1.5361,\n",
      "          1.1604,  0.9798,  0.9741,  0.9958,  0.9351,  1.0972,  0.5383,  0.7739,\n",
      "          0.5192,  1.4229,  0.7503,  1.8669,  1.0348,  0.5925,  0.8916,  0.9263,\n",
      "          1.9644,  0.6411,  0.6343,  1.5848,  0.8655,  1.1818,  0.8793,  1.4530,\n",
      "          0.9464,  1.2262],\n",
      "        [ 1.5366,  1.1967,  1.1098,  0.8639,  1.6153,  1.7524,  1.1198,  1.1297,\n",
      "          0.7748,  1.0113,  0.6804,  1.3027,  0.9974,  1.4765,  0.6600,  0.8062,\n",
      "          1.0788,  0.9907,  1.7185,  0.7997,  1.5773,  1.8011,  1.0211,  1.3464,\n",
      "          1.1691,  1.1230,  0.8060,  0.9062,  1.0608,  1.0642,  0.5162,  0.5950,\n",
      "          0.3727,  1.4240,  0.7236,  1.8883,  0.8696,  0.6125,  0.8155,  0.7598,\n",
      "          1.8282,  0.7009,  0.5921,  1.4706,  0.8728,  1.0686,  0.8170,  1.5191,\n",
      "          1.0469,  1.1284],\n",
      "        [ 0.3588,  0.4352, -2.0879,  0.0512,  0.4527,  0.5393, -2.6304,  0.5208,\n",
      "         -0.3722, -2.7530, -0.3099,  0.7718,  0.2092,  0.4319, -0.2189, -0.1109,\n",
      "         -1.7710, -0.0156, -1.5615,  0.0657,  0.1822, -0.2313,  0.1819,  0.5672,\n",
      "         -2.5489, -2.7455, -2.8526,  0.4133,  0.1816,  0.1072, -1.1330, -0.4517,\n",
      "         -0.7848, -2.9527, -1.9234,  0.8472,  0.1238, -0.3279, -0.0355, -0.0902,\n",
      "         -0.1162, -0.0683, -0.3192,  1.0339,  0.1465,  0.5195, -0.0288, -2.8443,\n",
      "          0.0642,  0.5394],\n",
      "        [ 0.3247,  0.5873, -2.1782,  0.0660,  0.5400,  0.5849, -2.9411,  0.3484,\n",
      "         -0.2962, -2.8148, -0.3841,  0.7472,  0.0954,  0.5182, -0.0199,  0.0221,\n",
      "         -2.0889,  0.1671, -1.7044,  0.0487,  0.3098,  0.0356,  0.1515,  0.5687,\n",
      "         -2.6919, -3.0164, -2.8723,  0.3267,  0.2430,  0.0906, -1.1189, -0.2514,\n",
      "         -0.8823, -3.0205, -2.2694,  0.7798,  0.0893, -0.5291,  0.1122, -0.1570,\n",
      "          0.0235, -0.3411, -0.2500,  0.9201,  0.2042,  0.3320, -0.0917, -2.9738,\n",
      "          0.0081,  0.5773],\n",
      "        [ 1.6172,  1.3803,  1.0122,  1.0700,  1.5042,  1.9373,  1.2449,  1.2681,\n",
      "          0.7160,  1.1830,  0.7483,  1.3933,  1.0326,  1.2740,  0.7462,  0.9488,\n",
      "          0.9872,  0.7832,  1.9370,  0.7421,  1.5386,  1.7365,  1.0714,  1.3092,\n",
      "          1.0371,  1.0463,  1.0234,  1.1173,  0.9532,  1.2540,  0.5440,  0.6923,\n",
      "          0.4924,  1.3818,  0.7149,  1.8566,  0.9286,  0.7944,  1.0316,  0.9458,\n",
      "          1.7638,  0.7591,  0.7682,  1.6797,  0.8961,  1.1980,  0.7921,  1.4609,\n",
      "          1.1865,  1.1697],\n",
      "        [ 0.4119,  0.5754, -1.8467,  0.2211,  0.3587,  0.3155, -2.5207,  0.3405,\n",
      "         -0.3298, -2.6204, -0.3204,  0.7300,  0.1047,  0.4059,  0.0216, -0.1189,\n",
      "         -1.8514,  0.0571, -1.6603,  0.0875,  0.1516, -0.1226,  0.3715,  0.4195,\n",
      "         -2.4502, -2.5872, -2.6478,  0.3499,  0.3908,  0.1041, -0.8540, -0.3911,\n",
      "         -0.8164, -2.9186, -2.0364,  0.7952,  0.1157, -0.2935, -0.0202, -0.0781,\n",
      "         -0.0494, -0.2903, -0.2513,  0.9914,  0.0809,  0.4827, -0.0855, -2.7726,\n",
      "          0.0130,  0.4718],\n",
      "        [ 0.4787,  0.5822, -2.1937,  0.1321,  0.5541,  0.6011, -2.6969,  0.4794,\n",
      "         -0.1242, -2.7567, -0.3360,  0.7018,  0.1307,  0.5779, -0.2689, -0.0313,\n",
      "         -1.9970,  0.0706, -1.4952,  0.0245,  0.1419, -0.0580,  0.2857,  0.4913,\n",
      "         -2.6387, -2.8234, -3.0200,  0.2274,  0.0840,  0.1373, -0.9476, -0.2535,\n",
      "         -0.8397, -3.0886, -2.1817,  0.8318, -0.0148, -0.5068, -0.0125, -0.0232,\n",
      "          0.0695, -0.0843, -0.2973,  0.8346,  0.0860,  0.2153, -0.1036, -3.0391,\n",
      "          0.2749,  0.5075],\n",
      "        [ 1.4624,  1.1458,  1.0642,  0.8846,  1.6506,  1.7984,  1.0808,  1.2465,\n",
      "          0.7535,  0.8892,  0.5576,  1.3444,  1.0106,  1.4006,  0.7320,  0.8664,\n",
      "          1.0612,  1.0070,  1.6383,  0.6938,  1.5688,  1.6688,  0.8635,  1.3700,\n",
      "          1.0696,  1.1853,  0.9767,  1.0958,  0.9543,  1.2597,  0.6683,  0.8019,\n",
      "          0.4014,  1.4005,  0.5433,  1.7008,  0.8607,  0.7045,  0.8101,  0.7525,\n",
      "          1.6664,  0.7788,  0.5659,  1.5267,  1.0148,  1.0771,  0.9425,  1.3655,\n",
      "          0.9957,  1.2925],\n",
      "        [ 1.3144,  1.1624,  0.9505,  0.9465,  1.7116,  1.7374,  1.0582,  1.1475,\n",
      "          0.7939,  0.9536,  0.5482,  1.1423,  0.9376,  1.4049,  0.6827,  0.9142,\n",
      "          0.9403,  0.9192,  1.6926,  0.8804,  1.5485,  1.7385,  1.0486,  1.3808,\n",
      "          1.0168,  0.9550,  1.0529,  0.9606,  0.9139,  1.1021,  0.6126,  0.7451,\n",
      "          0.3365,  1.3219,  0.6023,  1.7375,  0.9031,  0.6077,  0.8364,  0.8871,\n",
      "          1.6653,  0.8294,  0.7218,  1.6140,  0.9560,  0.9861,  0.9609,  1.5644,\n",
      "          0.8769,  1.1400]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1.3712, 0.6512, 1.0042, 0.5709, 1.2926, 0.6448, 0.5271, 0.2758, 1.1834,\n",
      "        1.1313, 1.2712, 1.1380, 1.0971, 0.3153, 0.7692, 1.3942, 0.5371, 0.0983,\n",
      "        1.0655, 1.1729], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.4017, -6.8699,  1.2680, -6.4331,  1.1642, -6.2867, -6.5038, -6.4106,\n",
      "          1.3250,  1.2499,  1.2125,  1.2299,  1.2459, -6.5626, -6.8067,  1.1372,\n",
      "         -6.5172, -6.6171,  1.3041,  1.3307]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1.0264], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
